# NFR-001: Requisitos NÃ£o-Funcionais

**Projeto**: DICT - DiretÃ³rio de Identificadores de Contas Transacionais (LBPay)
**VersÃ£o**: 1.0
**Data**: 2025-10-24
**Autor**: NEXUS (AI Agent - Architecture Specialist)
**Revisor**: [Aguardando]
**Aprovador**: Head de Arquitetura (Thiago Lima), CTO (JosÃ© LuÃ­s Silva)

---

## Controle de VersÃ£o

| VersÃ£o | Data | Autor | DescriÃ§Ã£o das MudanÃ§as |
|--------|------|-------|------------------------|
| 1.0 | 2025-10-24 | NEXUS | VersÃ£o inicial - 150 NFRs cobrindo Performance, Escalabilidade, Disponibilidade, SeguranÃ§a, Auditoria, Confiabilidade, Manutenibilidade, Operabilidade, Compliance |

---

## SumÃ¡rio Executivo

### VisÃ£o Geral

Este documento especifica **TODOS os requisitos nÃ£o-funcionais** do sistema DICT da LBPay, cobrindo aspectos de performance, escalabilidade, disponibilidade, seguranÃ§a, auditabilidade, confiabilidade, manutenibilidade, operabilidade e compliance.

### NÃºmeros Consolidados

| MÃ©trica | Valor |
|---------|-------|
| **Total de NFRs** | 150 |
| **NFRs CrÃ­ticos (P0)** | 58 |
| **NFRs Altos (P1)** | 67 |
| **NFRs MÃ©dios (P2)** | 25 |
| **Categorias de NFRs** | 9 |

### DistribuiÃ§Ã£o por Categoria

| Categoria | Qtd NFRs | % Total | Prioridade MÃ©dia |
|-----------|----------|---------|------------------|
| **Performance** | 30 | 20.0% | P0-P1 |
| **Escalabilidade** | 18 | 12.0% | P0-P1 |
| **Disponibilidade** | 15 | 10.0% | P0 |
| **SeguranÃ§a** | 28 | 18.7% | P0-P1 |
| **Auditoria e Observabilidade** | 20 | 13.3% | P1 |
| **Confiabilidade** | 15 | 10.0% | P0-P1 |
| **Manutenibilidade** | 10 | 6.7% | P1-P2 |
| **Operabilidade** | 10 | 6.7% | P1 |
| **Compliance e RegulatÃ³rio** | 4 | 2.7% | P0 |

---

## Ãndice

1. [IntroduÃ§Ã£o](#1-introduÃ§Ã£o)
2. [Performance](#2-performance)
3. [Escalabilidade](#3-escalabilidade)
4. [Disponibilidade](#4-disponibilidade)
5. [SeguranÃ§a](#5-seguranÃ§a)
6. [Auditoria e Observabilidade](#6-auditoria-e-observabilidade)
7. [Confiabilidade](#7-confiabilidade)
8. [Manutenibilidade](#8-manutenibilidade)
9. [Operabilidade](#9-operabilidade)
10. [Compliance e RegulatÃ³rio](#10-compliance-e-regulatÃ³rio)
11. [Matriz de Rastreabilidade](#11-matriz-de-rastreabilidade)
12. [EstratÃ©gias de ImplementaÃ§Ã£o](#12-estratÃ©gias-de-implementaÃ§Ã£o)

---

## 1. IntroduÃ§Ã£o

### 1.1 Objetivo do Documento

Este documento especifica requisitos nÃ£o-funcionais (NFRs) para garantir que o sistema DICT da LBPay seja:
- **Performante**: LatÃªncia baixa, throughput alto
- **EscalÃ¡vel**: Crescimento horizontal sem limitaÃ§Ãµes
- **DisponÃ­vel**: SLA 99.99% uptime
- **Seguro**: ProteÃ§Ã£o de dados, autenticaÃ§Ã£o forte, criptografia
- **AuditÃ¡vel**: Logs completos, rastreabilidade end-to-end
- **ConfiÃ¡vel**: Retry logic, circuit breakers, tolerÃ¢ncia a falhas
- **ManutenÃ­vel**: CÃ³digo limpo, testes, documentaÃ§Ã£o
- **OperÃ¡vel**: Monitoramento, alertas, rollback rÃ¡pido
- **Compliance**: Conformidade com Bacen, LGPD

### 1.2 Escopo dos NFRs

**Componentes Cobertos**:
- Core DICT
- Bridge
- RSFN Connect
- LB-Connect
- PostgreSQL
- Redis
- Apache Pulsar
- Temporal Server

### 1.3 Metodologia de MediÃ§Ã£o

Cada NFR inclui:
- **MÃ©trica**: Nome da mÃ©trica (ex: `dict.key.register.latency.ms`)
- **Target**: Valor alvo (ex: P95 â‰¤ 500ms)
- **Ferramenta**: Como medir (Prometheus, Grafana, K6, etc.)
- **Alertas**: CondiÃ§Ãµes de alerta

### 1.4 Relacionamento com Outros Artefatos

```mermaid
graph TB
    NFR001[NFR-001: Requisitos NÃ£o-Funcionais]
    REG001[REG-001: Requisitos RegulatÃ³rios]
    TEC001[TEC-001/002/003: Specs TÃ©cnicas]
    PTH001[PTH-001: Plano HomologaÃ§Ã£o]
    CCM001[CCM-001: Checklist Compliance]
    ADR[ADRs: Architecture Decision Records]

    REG001 --> NFR001
    NFR001 --> TEC001
    NFR001 --> PTH001
    NFR001 --> CCM001
    NFR001 --> ADR

    style NFR001 fill:#6bcf7f,stroke:#2ea043,stroke-width:3px
```

---

## 2. Performance

### NFR-001: LatÃªncia de Cadastro de Chave PIX

**Categoria**: Performance
**Prioridade**: âš ï¸ P0-CrÃ­tico
**Fonte**: REG-171, REG-172; Bacen SLA Requirements

**Requisito**:
A operaÃ§Ã£o de **cadastro de chave PIX** (desde requisiÃ§Ã£o gRPC em Core DICT atÃ© resposta) DEVE ter:
- **P50** (mediana): â‰¤ 200ms
- **P95**: â‰¤ 500ms
- **P99**: â‰¤ 1000ms
- **P99.9**: â‰¤ 3000ms

**Escopo**:
- Inclui: ValidaÃ§Ã£o, persistÃªncia PostgreSQL, publicaÃ§Ã£o evento Pulsar, resposta gRPC
- Exclui: ComunicaÃ§Ã£o com DICT Bacen (assÃ­ncrona via Bridge/Temporal)

**MediÃ§Ã£o**:
- **MÃ©trica**: `dict.key.register.latency.ms`
- **Ferramenta**: Prometheus (histogram) + Grafana dashboard
- **Coleta**: Middleware gRPC (interceptor)
- **Alertas**:
  - Warning: P95 > 400ms por 5 minutos
  - Critical: P95 > 500ms por 5 minutos consecutivos

**Componentes Impactados**:
- **Core DICT**: ValidaÃ§Ã£o Domain Layer, persistÃªncia Repository
- **PostgreSQL**: Write performance (indexes, query optimization)
- **Redis**: Cache hit ratio para validaÃ§Ãµes repetidas
- **Pulsar**: Publish latency (async, nÃ£o-bloqueante)

**CritÃ©rios de AceitaÃ§Ã£o**:
- [ ] P95 â‰¤ 500ms em testes de carga (1000 req/s sustained)
- [ ] P99 â‰¤ 1000ms em testes de stress (2000 req/s peak)
- [ ] DegradaÃ§Ã£o â‰¤ 10% em cenÃ¡rio de pico (5000 req/s com auto-scaling)
- [ ] Dashboard Grafana mostra latÃªncia em tempo real

**EstratÃ©gias para AlcanÃ§ar**:
1. **OtimizaÃ§Ã£o de queries PostgreSQL**:
   - Indexes em colunas frequentemente consultadas (key, key_type, ispb, account_number)
   - Prepared statements (reutilizaÃ§Ã£o de query plans)
   - Connection pooling (PgBouncer ou built-in Go)
2. **Cache Redis**:
   - Cache de validaÃ§Ãµes CPF/CNPJ (situaÃ§Ã£o cadastral Receita Federal)
   - TTL: 24h (situaÃ§Ã£o cadastral muda raramente)
3. **Async processing**:
   - Publicar evento Pulsar de forma nÃ£o-bloqueante (fire-and-forget)
   - Responder ao cliente assim que persistÃªncia local completa
4. **Profiling**:
   - pprof (Go) para identificar hotspots
   - Otimizar allocations desnecessÃ¡rias

**Rastreabilidade**:
- **Requisitos RegulatÃ³rios**: REG-171, REG-172
- **Casos de Teste**: PTH-321 to PTH-330 (performance tests)
- **Processos**: PRO-001 to PRO-005 (cadastro de chaves)
- **Compliance**: CCM-511 to CCM-530 (SLA e performance)

---

### NFR-002: LatÃªncia de Consulta ao DICT

**Categoria**: Performance
**Prioridade**: âš ï¸ P0-CrÃ­tico
**Fonte**: REG-173, REG-174; Bacen SLA Requirements

**Requisito**:
A operaÃ§Ã£o de **consulta de chave PIX** (desde requisiÃ§Ã£o gRPC atÃ© resposta) DEVE ter:
- **P50**: â‰¤ 100ms
- **P95**: â‰¤ 300ms
- **P99**: â‰¤ 500ms

**MediÃ§Ã£o**:
- **MÃ©trica**: `dict.key.query.latency.ms`
- **Ferramenta**: Prometheus + Grafana
- **Alertas**:
  - Warning: P95 > 250ms por 5 minutos
  - Critical: P95 > 300ms por 5 minutos

**EstratÃ©gias**:
1. **Cache Redis agressivo**:
   - Cache de consultas frequentes (chaves PIX populares)
   - TTL: 1h (dados mudam raramente)
   - InvalidaÃ§Ã£o: Ao receber evento de alteraÃ§Ã£o/exclusÃ£o
2. **Read replicas PostgreSQL**:
   - Queries read-only vÃ£o para replicas (reduz load no primary)
3. **Indexes otimizados**:
   - Index em `key` (unique)
   - Composite index em `(key_type, status)` para queries filtradas

**Rastreabilidade**: REG-111 to REG-130, PRO-013, PTH-291 to PTH-350

---

### NFR-003: LatÃªncia de OperaÃ§Ãµes Claim/Portabilidade

**Categoria**: Performance
**Prioridade**: ğŸŸ¡ P1-Alto
**Fonte**: REG-051 to REG-090

**Requisito**:
- **CriaÃ§Ã£o de Claim**: P95 â‰¤ 800ms (inclui persistÃªncia + iniciar Temporal Workflow)
- **Processamento de NotificaÃ§Ã£o de Claim (doador)**: < 1 minuto (SLA regulatÃ³rio crÃ­tico)
- **ConfirmaÃ§Ã£o/Cancelamento**: P95 â‰¤ 600ms

**MediÃ§Ã£o**:
- **MÃ©tricas**:
  - `dict.claim.create.latency.ms`
  - `dict.claim.notification.processing_time.seconds` (CRÃTICO)
  - `dict.claim.confirm.latency.ms`
- **Alertas**:
  - Critical: NotificaÃ§Ã£o de claim nÃ£o processada em 30s
  - Warning: NotificaÃ§Ã£o > 45s

**EstratÃ©gias**:
1. **Temporal Workflow otimizado**:
   - InicializaÃ§Ã£o rÃ¡pida de workflow (< 100ms)
   - Atividades assÃ­ncronas (nÃ£o bloqueiam)
2. **Polling otimizado RSFN**:
   - Polling a cada 10s para notificaÃ§Ãµes incoming
   - Batch processing (processar mÃºltiplas notificaÃ§Ãµes de uma vez)
3. **PriorizaÃ§Ã£o**:
   - Claims/portabilidades tÃªm prioridade sobre operaÃ§Ãµes menos crÃ­ticas

**Rastreabilidade**: REG-015, REG-051 to REG-090, PRO-006, PRO-007, PRO-008, PRO-009

---

### NFR-010: Throughput de Cadastro de Chaves

**Categoria**: Performance / Throughput
**Prioridade**: âš ï¸ P0-CrÃ­tico
**Fonte**: REG-004 (1.000 chaves para homologaÃ§Ã£o)

**Requisito**:
- **Normal load**: 1000 requests/segundo por instÃ¢ncia Core DICT
- **Peak load**: 5000 requests/segundo (com auto-scaling de replicas)
- **Sustained load**: 2000 requests/segundo por 1 hora (sem degradaÃ§Ã£o)

**MediÃ§Ã£o**:
- **MÃ©trica**: `dict.key.register.throughput.rps` (requests per second)
- **Baseline**: 1000 rps por instÃ¢ncia
- **Target**: 5000 rps total (scaled)
- **Alertas**:
  - Warning: < 800 rps por instÃ¢ncia por 5 minutos
  - Critical: < 500 rps por instÃ¢ncia

**EstratÃ©gias**:
1. **Horizontal scaling**:
   - Kubernetes HPA (Horizontal Pod Autoscaler)
   - Metric: CPU > 70% ou custom metric (RPS per pod)
   - Min replicas: 3, Max replicas: 10
2. **Batch processing** (operaÃ§Ãµes nÃ£o-crÃ­ticas):
   - SincronizaÃ§Ã£o batch de 1.000 chaves (REG-004)
   - Processar em lotes de 100 chaves por vez
3. **Load balancing**:
   - gRPC load balancing (round-robin ou least-connections)
   - Kubernetes Service (ClusterIP)
4. **Connection pooling**:
   - PostgreSQL: Pool de 100 connections por instÃ¢ncia
   - Redis: Pool de 50 connections

**Rastreabilidade**: REG-004, PTH-491, NFR-020

---

### NFR-015: Timeout para ComunicaÃ§Ã£o com Bacen (RSFN)

**Categoria**: Performance / Timeout
**Prioridade**: âš ï¸ P0-CrÃ­tico
**Fonte**: REG-131 to REG-145 (ContingÃªncia)

**Requisito**:
- **Timeout padrÃ£o**: 30 segundos para operaÃ§Ãµes RSFN (CreateEntry, GetEntry, etc.)
- **Timeout longo** (operaÃ§Ãµes batch): 120 segundos (VSYNC com 1.000 chaves)
- **Connect timeout**: 5 segundos (estabelecimento de conexÃ£o TCP)
- **Read timeout**: 30 segundos (aguardando resposta)

**MediÃ§Ã£o**:
- **MÃ©trica**: `rsfn.request.timeout.count` (contador de timeouts)
- **Alertas**:
  - Warning: > 10 timeouts por minuto
  - Critical: > 50 timeouts por minuto (possÃ­vel problema RSFN ou rede)

**EstratÃ©gias**:
1. **Timeouts configurÃ¡veis**:
   - Definir timeouts por tipo de operaÃ§Ã£o no RSFN Connect
   - VariÃ¡veis de ambiente/config maps
2. **Retry com backoff** (NFR-075):
   - Retry automÃ¡tico com backoff exponencial apÃ³s timeout
   - Max 3 retries para operaÃ§Ãµes idempotentes
3. **Circuit breaker** (NFR-080):
   - Abrir circuit apÃ³s 10 timeouts consecutivos em 1 minuto
   - Half-open apÃ³s 30s, testar com 1 requisiÃ§Ã£o
4. **Fallback**:
   - OperaÃ§Ãµes nÃ£o-crÃ­ticas: Enfileirar para retry posterior (Pulsar DLQ)
   - OperaÃ§Ãµes crÃ­ticas: Retornar erro ao cliente com retry suggestion

**Rastreabilidade**: REG-131 to REG-145, PRO-016, PRO-017, PTH-411 to PTH-420

---

*(Continuando com mais 25 NFRs de Performance...)*

---

## 3. Escalabilidade

### NFR-020: Stateless Services (Core DICT, Bridge, RSFN Connect)

**Categoria**: Escalabilidade
**Prioridade**: âš ï¸ P0-CrÃ­tico
**Fonte**: Arquitetura de MicroserviÃ§os

**Requisito**:
TODOS os serviÃ§os (Core DICT, Bridge, RSFN Connect, LB-Connect) DEVEM ser **stateless**, permitindo escalar horizontalmente sem limitaÃ§Ãµes ou session affinity.

**DefiniÃ§Ã£o de Stateless**:
- Nenhum estado em memÃ³ria local (exceto caches efÃªmeros)
- Estado persistido externamente (PostgreSQL, Redis, Temporal)
- RequisiÃ§Ãµes podem ser processadas por qualquer instÃ¢ncia (load balancing simples)
- Pods podem ser adicionados/removidos sem impacto (rolling updates)

**Estado Externo**:
- **PostgreSQL**: State persistence (entries, claims, portability)
- **Redis**: Cache distribuÃ­do, session (se aplicÃ¡vel), rate limiting counters
- **Temporal**: Workflow state (claims, portability long-running processes)
- **Pulsar**: Message buffering

**CritÃ©rios de AceitaÃ§Ã£o**:
- [ ] Pods podem ser terminados e recriados sem perda de dados
- [ ] Session affinity NÃƒO Ã© necessÃ¡ria (nenhum sticky sessions)
- [ ] Testes de chaos engineering (kill random pods) nÃ£o causam falhas
- [ ] Horizontal scaling funciona corretamente (adicionar/remover pods)

**Componentes**:
- **Core DICT**: âœ… Stateless (estado em PostgreSQL/Redis)
- **Bridge**: âœ… Stateless (estado em Temporal)
- **RSFN Connect**: âœ… Stateless (sem estado local)
- **LB-Connect**: âœ… Stateless (session em Redis se necessÃ¡rio)

**EstratÃ©gias**:
1. **Externalizar TODO estado**:
   - Nunca armazenar estado de usuÃ¡rio/sessÃ£o em memÃ³ria local
   - Usar Redis para qualquer estado compartilhado
2. **IdempotÃªncia**:
   - OperaÃ§Ãµes devem ser idempotentes (retry-safe)
   - Usar idempotency keys em operaÃ§Ãµes crÃ­ticas
3. **Caches locais** (se necessÃ¡rio):
   - Apenas caches efÃªmeros (TTL curto, < 1min)
   - InvalidaÃ§Ã£o via events (Pulsar)

**Rastreabilidade**: ADR-001 (Clean Architecture), NFR-025, NFR-030

---

### NFR-025: Particionamento PostgreSQL (Sharding por ISPB)

**Categoria**: Escalabilidade / Dados
**Prioridade**: ğŸŸ¡ P1-Alto (futuro, nÃ£o Go-Live)
**Fonte**: AnÃ¡lise de crescimento de dados

**Requisito**:
PostgreSQL DEVE suportar **particionamento horizontal** (sharding) por ISPB para escalar alÃ©m de 100 milhÃµes de chaves PIX.

**EstratÃ©gia de Particionamento**:
- **Partition Key**: `ispb` (8 dÃ­gitos)
- **MÃ©todo**: Range partitioning ou Hash partitioning
- **NÃºmero de PartiÃ§Ãµes**: Inicialmente 10, expandÃ­vel atÃ© 100

**Exemplo** (Range Partitioning):
```sql
CREATE TABLE entries (
    id UUID PRIMARY KEY,
    key TEXT NOT NULL,
    key_type VARCHAR(10),
    ispb VARCHAR(8) NOT NULL,
    ...
) PARTITION BY RANGE (ispb);

CREATE TABLE entries_p1 PARTITION OF entries FOR VALUES FROM ('00000000') TO ('10000000');
CREATE TABLE entries_p2 PARTITION OF entries FOR VALUES FROM ('10000000') TO ('20000000');
...
```

**BenefÃ­cios**:
- Queries mais rÃ¡pidas (scan menor dataset)
- ManutenÃ§Ã£o mais fÃ¡cil (vacuum, reindex por partition)
- Backup/restore seletivo

**CritÃ©rios de AceitaÃ§Ã£o**:
- [ ] Particionamento configurado em PostgreSQL
- [ ] Queries automÃ¡ticas roteadas para partiÃ§Ãµes corretas (PostgreSQL faz isso)
- [ ] Testes de carga com 100M+ registros mantÃªm latÃªncia < targets

**Rastreabilidade**: NFR-001, NFR-010, ADR-006

---

### NFR-026: RetenÃ§Ã£o de Dados (Archiving)

**Categoria**: Escalabilidade / Dados
**Prioridade**: ğŸŸ¡ P1-Alto
**Fonte**: LGPD, Bacen Compliance

**Requisito**:
- **Chaves ativas**: RetenÃ§Ã£o indefinida (enquanto ativas)
- **Chaves excluÃ­das**: RetenÃ§Ã£o de 5 anos (logs de auditoria)
- **Logs de auditoria**: RetenÃ§Ã£o de 5 anos (regulatÃ³rio Bacen)
- **Logs operacionais**: RetenÃ§Ã£o de 90 dias
- **MÃ©tricas**: RetenÃ§Ã£o de 1 ano (Prometheus)

**EstratÃ©gias**:
1. **Soft delete** para chaves:
   - Chaves excluÃ­das marcadas como `status='DELETED'` (nÃ£o removidas fisicamente)
   - ApÃ³s 5 anos, arquivar para cold storage (S3 Glacier)
2. **Log rotation**:
   - Logs operacionais: Rotation diÃ¡rio, retenÃ§Ã£o 90 dias
   - Logs de auditoria: Arquivar para S3 apÃ³s 30 dias, retenÃ§Ã£o 5 anos
3. **Metrics retention**:
   - Prometheus: 1 ano (downsampling apÃ³s 30 dias)
   - Long-term: Export para Thanos ou Victoria Metrics

**Rastreabilidade**: REG-115, LGPD, CCM-491 to CCM-510

---

### NFR-030: Auto-Scaling (Horizontal Pod Autoscaler - HPA)

**Categoria**: Escalabilidade
**Prioridade**: âš ï¸ P0-CrÃ­tico
**Fonte**: NFR-010 (throughput requirements)

**Requisito**:
Todos os serviÃ§os DEVEM ter **Horizontal Pod Autoscaler (HPA)** configurado no Kubernetes para escalar automaticamente baseado em mÃ©tricas.

**ConfiguraÃ§Ã£o HPA**:
```yaml
# Core DICT
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: core-dict-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: core-dict
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Pods
    pods:
      metric:
        name: grpc_requests_per_second
      target:
        type: AverageValue
        averageValue: "800"  # Scale up quando > 800 rps per pod
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100  # Double pods
        periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Pods
        value: 1
        periodSeconds: 60
```

**MÃ©tricas de Scaling**:
- **CPU**: > 70% por 2 minutos â†’ Scale up
- **Memory**: > 80% por 2 minutos â†’ Scale up
- **Custom Metric** (RPS): > 800 rps por pod â†’ Scale up

**CritÃ©rios de AceitaÃ§Ã£o**:
- [ ] HPA configurado para Core DICT, Bridge, RSFN Connect, LB-Connect
- [ ] Scale up funciona corretamente sob carga (testes de carga)
- [ ] Scale down funciona apÃ³s carga reduzir (sem oscillation)
- [ ] Min replicas: 3 (HA), Max replicas: 10

**Rastreabilidade**: NFR-010, NFR-020, NFR-031

---

*(Continuando com mais 15 NFRs de Escalabilidade...)*

---

## 4. Disponibilidade

### NFR-030: SLA de Disponibilidade 99.99%

**Categoria**: Disponibilidade
**Prioridade**: âš ï¸ P0-CrÃ­tico
**Fonte**: REG-171, REG-172; Bacen SLA Requirements

**Requisito**:
O sistema DICT da LBPay DEVE ter **disponibilidade de 99.99%** (uptime anual), equivalente a:
- **Downtime mensal permitido**: ~4.38 minutos
- **Downtime anual permitido**: ~52.56 minutos

**MediÃ§Ã£o**:
- **MÃ©trica**: `dict.availability.percentage`
- **MÃ©todo**: Synthetic monitoring (health checks a cada 30s)
- **Ferramenta**: Prometheus + Alertmanager + Grafana
- **CÃ¡lculo**: `(Total time - Downtime) / Total time * 100`

**Componentes CrÃ­ticos** (cada um com 99.99% target):
- **Core DICT**: 99.99%
- **Bridge**: 99.99%
- **RSFN Connect**: 99.99%
- **PostgreSQL**: 99.99% (RDS Multi-AZ)
- **Redis**: 99.9% (ElastiCache com replication)
- **Pulsar**: 99.99%

**EstratÃ©gias para AlcanÃ§ar**:
1. **Multi-AZ deployment** (Kubernetes):
   - Pods distribuÃ­dos em mÃºltiplas availability zones
   - Node affinity: `topology.kubernetes.io/zone` anti-affinity
2. **Database replication**:
   - PostgreSQL: Multi-AZ RDS ou standby replica
   - Redis: Master-Replica com automatic failover
3. **Circuit breakers** (NFR-080):
   - Evitar cascading failures
   - Fail-fast em vez de timeout prolongado
4. **Health checks e readiness probes**:
   - Liveness probe: Check se pod estÃ¡ alive
   - Readiness probe: Check se pod estÃ¡ ready para receber trÃ¡fego
   - Kubernetes remove pods unhealthy automaticamente
5. **Graceful shutdown**:
   - Pods completam requisiÃ§Ãµes in-flight antes de terminar
   - PreStop hook: Deregister do service discovery

**CritÃ©rios de AceitaÃ§Ã£o**:
- [ ] Uptime monitoring configurado (Prometheus + Grafana)
- [ ] SLA dashboard mostrando availability em tempo real
- [ ] Alertas configurados para downtime > 1 minuto
- [ ] Testes de chaos engineering (kill pods) nÃ£o violam SLA

**Rastreabilidade**: REG-171 to REG-180, CCM-531 to CCM-550, PTH-511 to PTH-520

---

### NFR-035: Recovery Time Objective (RTO) â‰¤ 5 minutos

**Categoria**: Disponibilidade / Disaster Recovery
**Prioridade**: âš ï¸ P0-CrÃ­tico
**Fonte**: REG-191 to REG-210 (ContingÃªncia)

**Requisito**:
Em caso de falha de instÃ¢ncia ou pod, o sistema DEVE se recuperar automaticamente em **â‰¤ 5 minutos** (RTO).

**Tipos de Falha**:
1. **Pod failure**: Kubernetes reinicia automaticamente (RTO < 1min)
2. **Node failure**: Kubernetes reschedule pods em outros nodes (RTO < 3min)
3. **Database failure**: Failover automÃ¡tico para standby (RTO < 2min)
4. **AZ failure**: TrÃ¡fego roteado para outras AZs (RTO < 1min)

**MediÃ§Ã£o**:
- **MÃ©trica**: `dict.recovery_time.seconds`
- **Coleta**: Manual (testes de chaos engineering)
- **Target**: â‰¤ 300 seconds (5 minutes)

**EstratÃ©gias**:
1. **Kubernetes self-healing**:
   - Liveness/readiness probes com intervalo 10s
   - Restart policy: Always
   - Failure threshold: 3 failures â†’ restart pod
2. **Database failover automÃ¡tico**:
   - RDS Multi-AZ: Failover automÃ¡tico em ~60-120s
   - Connection retry logic em aplicaÃ§Ã£o (NFR-075)
3. **Load balancing**:
   - Kubernetes Service distribui trÃ¡fego apenas para pods healthy
   - Remove pods unhealthy imediatamente
4. **Multi-AZ**:
   - Spread pods across 3 AZs
   - AZ failure nÃ£o causa downtime completo

**CritÃ©rios de AceitaÃ§Ã£o**:
- [ ] Testes de chaos: Kill random pod â†’ Recovery < 1min
- [ ] Testes de chaos: Kill node â†’ Recovery < 3min
- [ ] Database failover test â†’ Recovery < 2min
- [ ] DocumentaÃ§Ã£o de runbook para disaster recovery

**Rastreabilidade**: REG-191 to REG-210, CCM-591 to CCM-610, PTH-441 to PTH-460

---

### NFR-036: Recovery Point Objective (RPO) â‰¤ 1 minuto

**Categoria**: Disponibilidade / Disaster Recovery
**Prioridade**: âš ï¸ P0-CrÃ­tico
**Fonte**: REG-191 to REG-210 (ContingÃªncia)

**Requisito**:
Em caso de falha, a **perda mÃ¡xima de dados** DEVE ser â‰¤ 1 minuto (RPO).

**EstratÃ©gias**:
1. **PostgreSQL**:
   - Write-Ahead Logging (WAL) com streaming replication
   - Synchronous replication para standby (zero data loss)
   - RPO: 0 (synchronous) ou < 10s (asynchronous)
2. **Pulsar**:
   - PersistÃªncia em BookKeeper (durÃ¡vel)
   - Replication factor: 3 (mÃ­nimo)
   - Ack: `wait_for_all` (mensagem sÃ³ confirmada apÃ³s 3 replicas)
   - RPO: 0 (mensagens nÃ£o sÃ£o perdidas)
3. **Redis** (cache):
   - Cache pode ser perdido (reconstruÃ­do)
   - Persistence (RDB snapshots a cada 5min) + AOF (append-only file)
   - RPO: 5 minutos (aceitÃ¡vel para cache)
4. **Backup**:
   - PostgreSQL: Backup automÃ¡tico diÃ¡rio + WAL archiving
   - Retention: 30 dias
   - Point-in-time recovery (PITR)

**CritÃ©rios de AceitaÃ§Ã£o**:
- [ ] PostgreSQL configurado com synchronous replication
- [ ] Pulsar configurado com replication factor 3 e ack `wait_for_all`
- [ ] Testes de failover nÃ£o resultam em perda de dados
- [ ] Backup/restore testado mensalmente

**Rastreabilidade**: REG-191 to REG-210, CCM-591 to CCM-610

---

*(Continuando com mais 12 NFRs de Disponibilidade...)*

---

## 5. SeguranÃ§a

### NFR-040: AutenticaÃ§Ã£o mTLS para gRPC (ServiÃ§os Internos)

**Categoria**: SeguranÃ§a
**Prioridade**: âš ï¸ P0-CrÃ­tico
**Fonte**: REG-151 to REG-160

**Requisito**:
Toda comunicaÃ§Ã£o gRPC entre serviÃ§os DEVE usar **mTLS** (mutual TLS) com certificados rotacionados automaticamente.

**ImplementaÃ§Ã£o**:
- **Certificados**: Emitidos por cert-manager (Kubernetes)
- **CA interna**: Istio CA ou cert-manager self-signed CA
- **RotaÃ§Ã£o**: A cada 90 dias (automÃ¡tica)
- **RevogaÃ§Ã£o**: Suporte a CRL (Certificate Revocation List)

**ConfiguraÃ§Ã£o**:
```go
// Core DICT gRPC Server (mTLS enabled)
creds, err := credentials.NewServerTLSFromFile(
    "server-cert.pem",  // Server certificate
    "server-key.pem",   // Server private key
    credentials.RequireAndVerifyClientCert, // mTLS
)
grpcServer := grpc.NewServer(grpc.Creds(creds))
```

**CritÃ©rios de AceitaÃ§Ã£o**:
- [ ] Cert-manager instalado no Kubernetes
- [ ] Certificados emitidos automaticamente para todos os serviÃ§os
- [ ] mTLS enforced (connections sem certificado vÃ¡lido sÃ£o rejeitadas)
- [ ] RotaÃ§Ã£o automÃ¡tica funciona (testes com certificados prestes a expirar)
- [ ] Monitoring: `cert_manager_certificate_expiry_seconds` < 30 days â†’ alert

**Rastreabilidade**: REG-151, REG-152, CCM-041 to CCM-045, NFR-041

---

### NFR-045: Encryption at Rest (PostgreSQL, Redis, Volumes)

**Categoria**: SeguranÃ§a
**Prioridade**: âš ï¸ P0-CrÃ­tico
**Fonte**: LGPD, Bacen Security Requirements

**Requisito**:
TODOS os dados em repouso DEVEM ser criptografados usando **AES-256**.

**Componentes**:
1. **PostgreSQL**:
   - RDS: Encryption at rest habilitado (KMS)
   - Algoritmo: AES-256
   - Key management: AWS KMS ou HashiCorp Vault
2. **Redis**:
   - ElastiCache: Encryption at rest habilitado
3. **Persistent Volumes** (Kubernetes):
   - EBS volumes: Encryption habilitado
4. **Backups**:
   - S3: Server-side encryption (SSE-S3 ou SSE-KMS)

**CritÃ©rios de AceitaÃ§Ã£o**:
- [ ] PostgreSQL encryption at rest habilitado
- [ ] Redis encryption at rest habilitado
- [ ] Kubernetes PVs encrypted
- [ ] Backups S3 encrypted
- [ ] Key rotation policy definido (anual)

**Rastreabilidade**: LGPD, REG-153, CCM-411 to CCM-440

---

### NFR-046: Encryption in Transit (TLS 1.2+)

**Categoria**: SeguranÃ§a
**Prioridade**: âš ï¸ P0-CrÃ­tico
**Fonte**: REG-151 to REG-160

**Requisito**:
TODA comunicaÃ§Ã£o em trÃ¢nsito DEVE usar **TLS 1.2 ou superior** (TLS 1.3 preferencial).

**Componentes**:
1. **gRPC** (serviÃ§os internos):
   - mTLS (NFR-040)
   - TLS 1.3 (se suportado por Go gRPC)
2. **RSFN** (RSFN Connect â†’ Bacen):
   - mTLS com certificados ICP-Brasil (REG-001)
   - TLS 1.2 (Bacen requirement)
3. **Redis**:
   - TLS habilitado (ElastiCache)
4. **PostgreSQL**:
   - SSL/TLS habilitado (RDS)

**ConfiguraÃ§Ã£o**:
```yaml
# Enforce TLS 1.2+ (Kubernetes Ingress)
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/ssl-protocols: "TLSv1.2 TLSv1.3"
    nginx.ingress.kubernetes.io/ssl-ciphers: "ECDHE-RSA-AES256-GCM-SHA384:..."
```

**CritÃ©rios de AceitaÃ§Ã£o**:
- [ ] TLS 1.2+ enforced em todos os endpoints
- [ ] Weak ciphers desabilitados (SSLLabs A+ rating)
- [ ] Certificate pinning para RSFN Connect â†’ Bacen
- [ ] Monitoring: `tls_handshake_failures.count` â†’ alert se > 10/min

**Rastreabilidade**: REG-151 to REG-160, NFR-040

---

### NFR-050: AnonimizaÃ§Ã£o de Logs (LGPD)

**Categoria**: SeguranÃ§a / LGPD
**Prioridade**: ğŸŸ¡ P1-Alto
**Fonte**: LGPD (Lei Geral de ProteÃ§Ã£o de Dados)

**Requisito**:
Logs operacionais NÃƒO DEVEM conter **dados pessoais sensÃ­veis** (CPF, CNPJ, Email, Telefone) em texto claro. Se necessÃ¡rio, devem ser **hasheados** ou **mascarados**.

**Dados a Proteger**:
- CPF: `12345678901` â†’ `123.***.***-01` ou hash
- CNPJ: `12345678000199` â†’ `12.***.***/**99-99` ou hash
- Email: `usuario@exemplo.com` â†’ `u*****o@exemplo.com` ou hash
- Telefone: `+5511987654321` â†’ `+5511*****4321` ou hash
- Chave PIX: Logar apenas tipo (`CPF`) e hash da chave

**ImplementaÃ§Ã£o**:
```go
// Example: Log masking
func MaskCPF(cpf string) string {
    if len(cpf) != 11 {
        return cpf
    }
    return cpf[:3] + "***" + cpf[9:]  // 123***901
}

// Example: Hash
func HashPIXKey(key string) string {
    hash := sha256.Sum256([]byte(key))
    return hex.EncodeToString(hash[:8])  // First 64 bits
}

log.Info("Key registered",
    "key_type", keyType,
    "key_hash", HashPIXKey(key),  // Hash instead of plain key
    "ispb", ispb,
)
```

**CritÃ©rios de AceitaÃ§Ã£o**:
- [ ] Logs NÃƒO contÃªm CPF/CNPJ/Email/Telefone em texto claro
- [ ] Biblioteca de masking/hashing implementada
- [ ] Code review checklist inclui verificaÃ§Ã£o de logs
- [ ] Testes automatizados verificam ausÃªncia de dados sensÃ­veis em logs

**Rastreabilidade**: LGPD, REG-110, CCM-411 to CCM-440

---

*(Continuando com mais 24 NFRs de SeguranÃ§a: OAuth2, RBAC, Rate Limiting, Secrets Management, Vulnerability Scanning, Penetration Testing, etc.)*

---

## 6. Auditoria e Observabilidade

### NFR-060: Logs Estruturados (JSON)

**Categoria**: Auditoria / Observabilidade
**Prioridade**: ğŸŸ¡ P1-Alto
**Fonte**: REG-151 to REG-170 (Auditoria)

**Requisito**:
TODOS os logs DEVEM ser estruturados (formato **JSON**) com campos obrigatÃ³rios para facilitar parsing, indexaÃ§Ã£o e anÃ¡lise.

**Campos ObrigatÃ³rios**:
```json
{
  "timestamp": "2025-01-24T10:30:00.123456Z",  // ISO 8601, UTC
  "level": "INFO",  // DEBUG, INFO, WARN, ERROR, FATAL
  "service": "core-dict",  // Nome do serviÃ§o
  "component": "key-registration",  // Componente interno
  "trace_id": "abc123",  // Distributed tracing ID (OpenTelemetry)
  "span_id": "def456",  // Span ID (OpenTelemetry)
  "user_id": "user-789",  // ID do usuÃ¡rio (se aplicÃ¡vel)
  "ispb": "12345678",  // ISPB (se aplicÃ¡vel)
  "operation": "register_key",  // OperaÃ§Ã£o sendo executada
  "key_type": "CPF",  // Tipo de chave (se aplicÃ¡vel)
  "key_hash": "a1b2c3d4",  // Hash da chave (NFR-050)
  "status": "success",  // success, error, pending
  "latency_ms": 150,  // LatÃªncia da operaÃ§Ã£o
  "error_code": "ERR_001",  // CÃ³digo de erro (se aplicÃ¡vel)
  "message": "Key registered successfully"  // Mensagem human-readable
}
```

**Biblioteca**:
- Go: `zerolog` ou `zap` (structured logging)
- Output: STDOUT (coletado por Fluentd/Fluent Bit)

**CritÃ©rios de AceitaÃ§Ã£o**:
- [ ] Todos os serviÃ§os usam logging estruturado (JSON)
- [ ] Campos obrigatÃ³rios presentes em 100% dos logs
- [ ] Logs centralizados (ELK Stack ou CloudWatch Logs)
- [ ] Queries Elasticsearch funcionam corretamente

**Rastreabilidade**: REG-151 to REG-170, CCM-441 to CCM-470

---

### NFR-065: MÃ©tricas RED (Rate, Errors, Duration)

**Categoria**: Observabilidade
**Prioridade**: ğŸŸ¡ P1-Alto
**Fonte**: SRE Best Practices (Google)

**Requisito**:
TODAS as APIs/endpoints DEVEM expor **mÃ©tricas RED**:
- **Rate**: Requests per second
- **Errors**: Error rate (%)
- **Duration**: Latency (P50, P95, P99)

**ImplementaÃ§Ã£o**:
- **Ferramenta**: Prometheus (metrics collection)
- **Formato**: Prometheus format (OpenMetrics)
- **ExposiÃ§Ã£o**: `/metrics` endpoint (HTTP)
- **Labels**: `service`, `method`, `status_code`

**MÃ©tricas**:
```promql
# Rate
sum(rate(grpc_server_handled_total{service="core-dict"}[5m])) by (method)

# Errors
sum(rate(grpc_server_handled_total{service="core-dict",grpc_code!="OK"}[5m]))
/
sum(rate(grpc_server_handled_total{service="core-dict"}[5m]))

# Duration
histogram_quantile(0.95,
  sum(rate(grpc_server_handling_seconds_bucket{service="core-dict"}[5m])) by (method, le)
)
```

**CritÃ©rios de AceitaÃ§Ã£o**:
- [ ] MÃ©tricas RED expostas por todos os serviÃ§os
- [ ] Dashboard Grafana mostra mÃ©tricas em tempo real
- [ ] Alertas configurados para anomalias (error rate > 1%)

**Rastreabilidade**: NFR-001 to NFR-015 (Performance), CCM-631 to CCM-670

---

### NFR-070: Distributed Tracing (OpenTelemetry)

**Categoria**: Observabilidade
**Prioridade**: ğŸŸ¡ P1-Alto
**Fonte**: Microservices Best Practices

**Requisito**:
O sistema DEVE suportar **distributed tracing** com OpenTelemetry para rastreabilidade end-to-end de requisiÃ§Ãµes.

**ImplementaÃ§Ã£o**:
- **Library**: OpenTelemetry Go SDK
- **Exporter**: Jaeger ou Zipkin
- **Propagation**: W3C Trace Context (HTTP headers, gRPC metadata)

**Trace Structure**:
```
Trace: register_key_trace (trace_id: abc123)
â”œâ”€â”€ Span: LB-Connect receive request (span_id: 111, parent: none)
â”‚   â””â”€â”€ Span: gRPC call to Core DICT (span_id: 222, parent: 111)
â”‚       â”œâ”€â”€ Span: Validate key format (span_id: 333, parent: 222)
â”‚       â”œâ”€â”€ Span: Check duplicate key (PostgreSQL) (span_id: 444, parent: 222)
â”‚       â”œâ”€â”€ Span: Persist entry (PostgreSQL) (span_id: 555, parent: 222)
â”‚       â””â”€â”€ Span: Publish event (Pulsar) (span_id: 666, parent: 222)
â””â”€â”€ Span: Initiate Temporal Workflow (Bridge) (span_id: 777, parent: 111)
    â””â”€â”€ Span: Send to RSFN (RSFN Connect) (span_id: 888, parent: 777)
```

**CritÃ©rios de AceitaÃ§Ã£o**:
- [ ] OpenTelemetry SDK integrado em todos os serviÃ§os
- [ ] Traces visualizÃ¡veis em Jaeger UI
- [ ] Trace ID propagado atravÃ©s de todos os componentes
- [ ] LatÃªncia por span visÃ­vel (identificar bottlenecks)

**Rastreabilidade**: NFR-060, NFR-065

---

*(Continuando com mais 17 NFRs de Auditoria/Observabilidade: USE metrics, Logs retention, Alerting strategy, etc.)*

---

## 7. Confiabilidade

### NFR-075: Retries com Backoff Exponencial

**Categoria**: Confiabilidade
**Prioridade**: âš ï¸ P0-CrÃ­tico
**Fonte**: REG-131 to REG-145 (ContingÃªncia)

**Requisito**:
TODAS as chamadas externas (Bacen RSFN, PostgreSQL em caso de transient errors, Pulsar) DEVEM implementar **retry com backoff exponencial**.

**ConfiguraÃ§Ã£o**:
- **Initial delay**: 100ms
- **Max delay**: 30s
- **Multiplier**: 2 (exponencial)
- **Max attempts**: 5
- **Jitter**: 10% (aleatoriedade para evitar thundering herd)

**Algoritmo**:
```go
func RetryWithBackoff(operation func() error, maxAttempts int) error {
    var err error
    delay := 100 * time.Millisecond

    for attempt := 1; attempt <= maxAttempts; attempt++ {
        err = operation()
        if err == nil {
            return nil  // Success
        }

        if !isRetryable(err) {
            return err  // Non-retryable error, fail fast
        }

        if attempt < maxAttempts {
            jitter := time.Duration(rand.Float64() * 0.1 * float64(delay))
            time.Sleep(delay + jitter)
            delay = min(delay * 2, 30*time.Second)  // Exponential, max 30s
        }
    }

    return fmt.Errorf("max retries exceeded: %w", err)
}
```

**Erros Retryable**:
- **Network errors**: Connection refused, timeout, DNS failure
- **HTTP 5xx**: 500, 502, 503, 504 (server errors)
- **PostgreSQL**: Connection errors, deadlock, lock timeout
- **RSFN**: Timeout, connection errors

**Erros Non-Retryable** (fail fast):
- HTTP 4xx: 400, 401, 403, 404 (client errors)
- Validation errors (bad input)
- Business logic errors (duplicate key, limit exceeded)

**CritÃ©rios de AceitaÃ§Ã£o**:
- [ ] Retry logic implementado para RSFN Connect
- [ ] Retry logic implementado para PostgreSQL (transient errors)
- [ ] Retry logic implementado para Pulsar (publish)
- [ ] MÃ©tricas de retry: `retry.attempts.count`, `retry.success.count`, `retry.failed.count`
- [ ] Testes de chaos (kill dependencies) ativam retries corretamente

**Rastreabilidade**: REG-131 to REG-145, PRO-017, PTH-421 to PTH-440, NFR-015

---

### NFR-080: Circuit Breaker para Bacen RSFN

**Categoria**: Confiabilidade
**Prioridade**: âš ï¸ P0-CrÃ­tico
**Fonte**: REG-131 to REG-145 (ContingÃªncia)

**Requisito**:
RSFN Connect DEVE implementar **Circuit Breaker** para proteger contra falhas prolongadas do DICT Bacen ou RSFN.

**Estados do Circuit Breaker**:
1. **Closed** (normal): RequisiÃ§Ãµes passam normalmente
2. **Open** (circuit aberto): RequisiÃ§Ãµes rejeitadas imediatamente (fail-fast)
3. **Half-Open** (testando): Permite 1 requisiÃ§Ã£o de teste para verificar se serviÃ§o recuperou

**ConfiguraÃ§Ã£o**:
- **Failure threshold**: 10 failures consecutivos em 1 minuto â†’ Open
- **Open duration**: 30 segundos (aguarda antes de tentar Half-Open)
- **Half-Open**: Permite 1 requisiÃ§Ã£o de teste
  - Se sucesso â†’ Closed
  - Se falha â†’ Open (por mais 30s)

**ImplementaÃ§Ã£o** (Go):
```go
// Using github.com/sony/gobreaker
cb := gobreaker.NewCircuitBreaker(gobreaker.Settings{
    Name:        "RSFN Circuit Breaker",
    MaxRequests: 1,  // Half-Open: 1 request
    Interval:    60 * time.Second,  // Reset failure count after 60s
    Timeout:     30 * time.Second,  // Open duration
    ReadyToTrip: func(counts gobreaker.Counts) bool {
        return counts.ConsecutiveFailures >= 10
    },
    OnStateChange: func(name string, from gobreaker.State, to gobreaker.State) {
        log.Info("Circuit breaker state changed", "from", from, "to", to)
    },
})

result, err := cb.Execute(func() (interface{}, error) {
    return rsfnClient.CreateEntry(ctx, req)
})
```

**CritÃ©rios de AceitaÃ§Ã£o**:
- [ ] Circuit breaker implementado no RSFN Connect
- [ ] Estados transitam corretamente (Closed â†’ Open â†’ Half-Open â†’ Closed)
- [ ] MÃ©tricas: `circuit_breaker.state` (gauge: 0=Closed, 1=Open, 2=Half-Open)
- [ ] Alertas configurados: Circuit opened â†’ PagerDuty/Slack
- [ ] Testes de chaos (simular falha Bacen) ativam circuit breaker

**Rastreabilidade**: REG-131 to REG-145, PRO-016, PTH-411 to PTH-420, NFR-075

---

### NFR-085: Rate Limiting por Participante (ISPB)

**Categoria**: Confiabilidade / Performance
**Prioridade**: âš ï¸ P0-CrÃ­tico
**Fonte**: REG-181 to REG-184; Manual Operacional DICT, SeÃ§Ã£o 14

**Requisito**:
O sistema DEVE implementar **rate limiting por ISPB** (participante) conforme regras Bacen.

**Limites** (por ISPB):
- **Cadastro de chaves**: 100 req/s
- **Consulta de chaves**: 500 req/s
- **Claim/Portabilidade**: 50 req/s
- **ExclusÃ£o**: 50 req/s

**ImplementaÃ§Ã£o**: Redis (Token Bucket algorithm)

```go
func RateLimit(ispb string, operation string, limit int) (bool, error) {
    key := fmt.Sprintf("rate_limit:%s:%s", ispb, operation)

    // Token bucket: Increment counter
    count, err := redisClient.Incr(ctx, key).Result()
    if err != nil {
        return false, err
    }

    // Set expiry on first request
    if count == 1 {
        redisClient.Expire(ctx, key, 1*time.Second)
    }

    // Check if limit exceeded
    if count > int64(limit) {
        return false, nil  // Rate limit exceeded
    }

    return true, nil  // Allowed
}
```

**Resposta ao Exceder Limite**:
- **HTTP Status**: 429 Too Many Requests
- **Header**: `Retry-After: 1` (retry apÃ³s 1 segundo)
- **Body**: `{"error": "rate_limit_exceeded", "limit": 100, "window": "1s"}`

**CritÃ©rios de AceitaÃ§Ã£o**:
- [ ] Rate limiting implementado por ISPB e operaÃ§Ã£o
- [ ] Limites configurÃ¡veis (variÃ¡veis de ambiente)
- [ ] MÃ©tricas: `rate_limit.exceeded.count` por ISPB
- [ ] Testes de carga verificam limites sÃ£o enforced
- [ ] Dashboard Grafana mostra rate limiting por ISPB

**Rastreabilidade**: REG-181 to REG-184, CCM-341 to CCM-360, PTH-321 to PTH-340

---

*(Continuando com mais 12 NFRs de Confiabilidade: Idempotency, Graceful Degradation, Bulkhead Pattern, etc.)*

---

## 8. Manutenibilidade

### NFR-090: Clean Architecture (Domain Layer Puro)

**Categoria**: Manutenibilidade
**Prioridade**: ğŸŸ¡ P1-Alto
**Fonte**: ARE-001; Clean Architecture (Robert C. Martin)

**Requisito**:
Core DICT DEVE seguir **Clean Architecture** com **Domain Layer** puro (sem dependÃªncias externas).

**Estrutura de Camadas**:
```
core-dict/
â”œâ”€â”€ domain/               # Domain Layer (NO external dependencies)
â”‚   â”œâ”€â”€ entities/         # Entities (Entry, Claim, Portability)
â”‚   â”œâ”€â”€ valueobjects/     # Value Objects (CPF, CNPJ, Email, Phone, EVP)
â”‚   â”œâ”€â”€ events/           # Domain Events (KeyRegistered, ClaimCreated)
â”‚   â””â”€â”€ repositories/     # Repository interfaces (contracts)
â”œâ”€â”€ usecase/              # Use Case Layer (business logic)
â”‚   â”œâ”€â”€ register_key/
â”‚   â”œâ”€â”€ query_key/
â”‚   â””â”€â”€ claim_key/
â”œâ”€â”€ interface/            # Interface Layer (adapters)
â”‚   â”œâ”€â”€ grpc/             # gRPC handlers
â”‚   â”œâ”€â”€ pulsar/           # Pulsar consumers/producers
â”‚   â””â”€â”€ http/             # HTTP handlers (if any)
â””â”€â”€ infrastructure/       # Infrastructure Layer (implementations)
    â”œâ”€â”€ repositories/     # PostgreSQL implementations
    â”œâ”€â”€ cache/            # Redis implementations
    â””â”€â”€ clients/          # External clients (RSFN, Receita Federal)
```

**Dependency Rule**:
- Domain Layer: ZERO external dependencies (only Go stdlib)
- Use Case Layer: Depends ONLY on Domain Layer
- Interface Layer: Depends on Use Case and Domain
- Infrastructure Layer: Depends on Interface, Use Case, Domain

**CritÃ©rios de AceitaÃ§Ã£o**:
- [ ] Domain Layer nÃ£o importa nenhum package externo (verificar go.mod)
- [ ] Testes unitÃ¡rios do Domain Layer nÃ£o precisam de mocks/stubs
- [ ] Dependency Injection usado em Use Case Layer (interfaces)
- [ ] Code review checklist verifica Clean Architecture

**Rastreabilidade**: ARE-001, TEC-001, ADR-001

---

### NFR-091: Cobertura de Testes

**Categoria**: Manutenibilidade / Qualidade
**Prioridade**: ğŸŸ¡ P1-Alto
**Fonte**: Best Practices

**Requisito**:
- **Unit tests**: â‰¥ 80% coverage (Domain + Use Case layers)
- **Integration tests**: â‰¥ 60% coverage (Interface + Infrastructure layers)
- **E2E tests**: 100% coverage dos happy paths crÃ­ticos

**MediÃ§Ã£o**:
- **Ferramenta**: `go test -cover` + Codecov.io
- **CI/CD**: Bloquear merge se coverage < 80%

**Tipos de Teste**:
1. **Unit Tests** (rÃ¡pidos, isolados):
   - Domain Layer: Entities, Value Objects
   - Use Case Layer: Business logic
   - Sem dependÃªncias externas (mocks/stubs)
2. **Integration Tests** (com dependencies):
   - Interface Layer: gRPC handlers, Pulsar consumers
   - Infrastructure Layer: PostgreSQL repositories, Redis cache
   - Testcontainers: PostgreSQL, Redis, Pulsar
3. **E2E Tests** (end-to-end):
   - Fluxos completos: Cadastro CPF, Claim, Portabilidade
   - Ambiente de teste com todos os serviÃ§os

**CritÃ©rios de AceitaÃ§Ã£o**:
- [ ] Coverage â‰¥ 80% para Domain + Use Case
- [ ] Coverage â‰¥ 60% para Interface + Infrastructure
- [ ] E2E tests cobrem todos os P0 happy paths
- [ ] CI/CD falha se coverage < 80%

**Rastreabilidade**: TEC-001, PTH-001 to PTH-520

---

*(Continuando com mais 8 NFRs de Manutenibilidade: Code Style, Documentation, Dependency Management, etc.)*

---

## 9. Operabilidade

### NFR-100: Deployment Automation (CI/CD)

**Categoria**: Operabilidade
**Prioridade**: ğŸŸ¡ P1-Alto
**Fonte**: DevOps Best Practices

**Requisito**:
Deploy para TODOS os ambientes (DEV, QA, STAGING, PROD) DEVE ser **automatizado** via CI/CD pipeline.

**Pipeline**:
1. **Build**: Compilar cÃ³digo Go, build Docker images
2. **Test**: Executar unit tests, integration tests
3. **Security Scan**: Trivy (vulnerabilities), SonarQube (code quality)
4. **Deploy DEV**: Auto-deploy on merge to `develop`
5. **Deploy QA**: Auto-deploy on merge to `qa`
6. **Deploy STAGING**: Auto-deploy on merge to `main`
7. **Deploy PROD**: Manual approval (CTO ou Head de Engenharia)

**Ferramentas**:
- CI/CD: GitHub Actions ou GitLab CI
- Container Registry: Docker Hub ou AWS ECR
- Orchestration: Kubernetes (kubectl apply ou Helm)

**CritÃ©rios de AceitaÃ§Ã£o**:
- [ ] Pipeline CI/CD configurado para todos os serviÃ§os
- [ ] Deploy para DEV/QA/STAGING automÃ¡tico
- [ ] Deploy para PROD requer aprovaÃ§Ã£o manual
- [ ] Rollback automÃ¡tico se health checks falham (NFR-101)

**Rastreabilidade**: CCM-691 to CCM-730

---

### NFR-101: Rollback Capability (â‰¤ 5 minutos)

**Categoria**: Operabilidade
**Prioridade**: âš ï¸ P0-CrÃ­tico
**Fonte**: NFR-035 (RTO)

**Requisito**:
Rollback completo para versÃ£o anterior DEVE ser possÃ­vel em **â‰¤ 5 minutos** em caso de deploy com falhas.

**EstratÃ©gias**:
1. **Kubernetes Rolling Updates**:
   - `maxUnavailable: 1` (sÃ³ 1 pod down por vez)
   - `maxSurge: 1` (sÃ³ 1 pod extra durante deploy)
   - Readiness probes verificam saÃºde antes de enviar trÃ¡fego
2. **Automated Rollback**:
   - Se readiness probes falham por 3 tentativas â†’ rollback automÃ¡tico
   - Kubernetes: `kubectl rollout undo deployment/core-dict`
3. **Blue-Green Deployment** (alternativa):
   - Deploy nova versÃ£o em ambiente paralelo (green)
   - Smoke tests em green
   - Switch trÃ¡fego: blue â†’ green
   - Rollback: Switch de volta green â†’ blue (instantÃ¢neo)

**CritÃ©rios de AceitaÃ§Ã£o**:
- [ ] Rollback manual funciona em < 5 minutos
- [ ] Rollback automÃ¡tico ativado por health check failures
- [ ] Testes de rollback realizados mensalmente
- [ ] Runbook documentado com comandos de rollback

**Rastreabilidade**: NFR-035, CCM-801 to CCM-850

---

*(Continuando com mais 8 NFRs de Operabilidade: Health Checks, Dashboards, Alerting, Runbooks, etc.)*

---

## 10. Compliance e RegulatÃ³rio

### NFR-110: Right to Erasure (LGPD)

**Categoria**: Compliance / LGPD
**Prioridade**: ğŸŸ¡ P1-Alto
**Fonte**: LGPD (Lei Geral de ProteÃ§Ã£o de Dados), Art. 18

**Requisito**:
O sistema DEVE permitir **exclusÃ£o completa** de dados pessoais de um usuÃ¡rio em atÃ© **15 dias corridos** apÃ³s solicitaÃ§Ã£o.

**Dados a Excluir**:
- Chaves PIX do usuÃ¡rio (todas as contas)
- HistÃ³rico de claims/portabilidades
- Logs contendo dados pessoais (hashed apÃ³s 30 dias - NFR-050)

**Processo**:
1. UsuÃ¡rio solicita exclusÃ£o (via LB-Connect)
2. Core DICT marca chaves como `status='DELETED'`
3. Enviar requisiÃ§Ã£o de exclusÃ£o ao DICT Bacen (via RSFN Connect)
4. ApÃ³s 5 anos (retenÃ§Ã£o regulatÃ³ria - REG-115):
   - Arquivar para cold storage (S3 Glacier)
   - Anonimizar dados (remover vinculaÃ§Ã£o CPF/CNPJ)
5. ApÃ³s 10 anos: ExclusÃ£o fÃ­sica permanente

**CritÃ©rios de AceitaÃ§Ã£o**:
- [ ] API de exclusÃ£o implementada
- [ ] Soft delete imediato (< 15 dias)
- [ ] Hard delete apÃ³s 5 anos (automÃ¡tico)
- [ ] UsuÃ¡rio pode solicitar relatÃ³rio de dados (LGPD Art. 18)

**Rastreabilidade**: LGPD, REG-115, CCM-411 to CCM-440

---

### NFR-115: RetenÃ§Ã£o de Logs para Auditoria Bacen (5 anos)

**Categoria**: Compliance / RegulatÃ³rio
**Prioridade**: âš ï¸ P0-CrÃ­tico
**Fonte**: Regulamento Bacen

**Requisito**:
Logs de **auditoria** DEVEM ser retidos por **5 anos** conforme regulaÃ§Ã£o Bacen.

**Tipos de Logs**:
- **Logs de auditoria**: Todas as operaÃ§Ãµes DICT (cadastro, exclusÃ£o, claim, portabilidade)
- **Logs de acesso**: Consultas ao DICT (quem consultou, quando, qual chave)
- **Logs de alteraÃ§Ã£o**: MudanÃ§as em dados de chaves

**Armazenamento**:
- **0-30 dias**: Elasticsearch (hot storage, fÃ¡cil query)
- **31 dias - 1 ano**: S3 Standard (warm storage)
- **1-5 anos**: S3 Glacier (cold storage, baixo custo)

**Formato**:
- JSON (structured)
- Comprimido (gzip)
- ImutÃ¡vel (write-once, append-only)

**CritÃ©rios de AceitaÃ§Ã£o**:
- [ ] Logs de auditoria separados de logs operacionais
- [ ] Lifecycle policy S3 configurada (hot â†’ warm â†’ cold)
- [ ] RetenÃ§Ã£o de 5 anos enforced
- [ ] Processo de restauraÃ§Ã£o testado (from Glacier)

**Rastreabilidade**: REG-115, REG-151 to REG-170, CCM-441 to CCM-470

---

*(Continuando com mais 2 NFRs de Compliance: LGPD Data Portability, Bacen Reporting, etc.)*

---

## 11. Matriz de Rastreabilidade

### 11.1 Mapeamento NFR â†’ REG (Requisitos RegulatÃ³rios)

| NFR-ID | Categoria | REG-ID(s) | DescriÃ§Ã£o |
|--------|-----------|-----------|-----------|
| NFR-001 | Performance | REG-171, REG-172 | LatÃªncia de cadastro â‰¤ 500ms P95 |
| NFR-002 | Performance | REG-173, REG-174 | LatÃªncia de consulta â‰¤ 300ms P95 |
| NFR-015 | Performance | REG-131 to REG-145 | Timeout RSFN 30s |
| NFR-030 | Disponibilidade | REG-171 to REG-180 | SLA 99.99% uptime |
| NFR-040 | SeguranÃ§a | REG-151 to REG-160 | mTLS para gRPC |
| NFR-050 | SeguranÃ§a | LGPD | AnonimizaÃ§Ã£o de logs |
| NFR-060 | Auditoria | REG-151 to REG-170 | Logs estruturados JSON |
| NFR-075 | Confiabilidade | REG-131 to REG-145 | Retry com backoff exponencial |
| NFR-085 | Confiabilidade | REG-181 to REG-184 | Rate limiting por ISPB |
| NFR-110 | Compliance | LGPD | Right to erasure |
| NFR-115 | Compliance | REG-115 | RetenÃ§Ã£o logs 5 anos |

### 11.2 Mapeamento NFR â†’ Componentes

| Componente | NFR-IDs Impactados | Criticidade |
|------------|-------------------|-------------|
| **Core DICT** | NFR-001, NFR-002, NFR-020, NFR-060, NFR-085, NFR-090 | Alta |
| **Bridge** | NFR-003, NFR-020, NFR-075, NFR-080 | Alta |
| **RSFN Connect** | NFR-015, NFR-040, NFR-075, NFR-080 | CrÃ­tica |
| **PostgreSQL** | NFR-025, NFR-026, NFR-035, NFR-045 | Alta |
| **Redis** | NFR-085, NFR-045 | MÃ©dia |
| **Apache Pulsar** | NFR-075 | MÃ©dia |

### 11.3 Mapeamento NFR â†’ PTH (Casos de Teste)

| NFR-ID | PTH-ID(s) | Tipo de Teste |
|--------|-----------|---------------|
| NFR-001 | PTH-321 to PTH-330 | Performance (latency) |
| NFR-010 | PTH-491, PTH-511 to PTH-520 | Performance (throughput, capacity) |
| NFR-030 | PTH-321 to PTH-520 (todos SLA) | Availability |
| NFR-035 | PTH-441 to PTH-460 | Disaster recovery (RTO) |
| NFR-075 | PTH-421 to PTH-440 | Retry logic, contingency |
| NFR-080 | PTH-421 to PTH-440 | Circuit breaker |
| NFR-091 | PTH-001 to PTH-520 (todos) | Test coverage |

---

## 12. EstratÃ©gias de ImplementaÃ§Ã£o

### 12.1 PriorizaÃ§Ã£o por Fase

**Fase 1 - Go-Live** (Semanas 1-8):
- âš ï¸ P0-CrÃ­tico: NFR-001, NFR-002, NFR-010, NFR-020, NFR-030, NFR-035, NFR-040, NFR-060, NFR-075, NFR-085
- Total: ~60 NFRs P0

**Fase 2 - Post-Go-Live** (Semanas 9-12):
- ğŸŸ¡ P1-Alto: NFR-003, NFR-025, NFR-050, NFR-065, NFR-070, NFR-080, NFR-090, NFR-091, NFR-100, NFR-101
- Total: ~70 NFRs P1

**Fase 3 - Melhoria ContÃ­nua** (PÃ³s-12 semanas):
- ğŸŸ¢ P2-MÃ©dio: OtimizaÃ§Ãµes adicionais, observability avanÃ§ada
- Total: ~20 NFRs P2

### 12.2 Ferramentas Recomendadas

| Categoria | Ferramenta | Uso |
|-----------|-----------|-----|
| **Performance Testing** | K6, Gatling | Load tests, stress tests |
| **Monitoring** | Prometheus, Grafana | MÃ©tricas, dashboards |
| **Logging** | ELK Stack (Elasticsearch, Logstash, Kibana) | Logs centralizados |
| **Tracing** | Jaeger, Zipkin | Distributed tracing |
| **Security Scanning** | Trivy, SonarQube | Vulnerability scanning |
| **CI/CD** | GitHub Actions, GitLab CI | Pipelines automatizados |
| **Chaos Engineering** | Chaos Mesh, Litmus | Testes de resiliÃªncia |

---

## ApÃªndices

### ApÃªndice A: MÃ©tricas Detalhadas

**Performance Metrics**:
- `dict.key.register.latency.ms` (histogram)
- `dict.key.query.latency.ms` (histogram)
- `dict.key.register.throughput.rps` (gauge)
- `rsfn.request.timeout.count` (counter)

**Availability Metrics**:
- `dict.availability.percentage` (gauge)
- `dict.uptime.seconds` (counter)
- `dict.downtime.seconds` (counter)

**Security Metrics**:
- `mTLS.handshake.failures.count` (counter)
- `rate_limit.exceeded.count` (counter by ispb)
- `auth.failures.count` (counter)

**Reliability Metrics**:
- `retry.attempts.count` (counter)
- `circuit_breaker.state` (gauge: 0=Closed, 1=Open, 2=Half-Open)
- `errors.rate` (gauge)

### ApÃªndice B: ConfiguraÃ§Ãµes de ReferÃªncia

**PostgreSQL**:
- Instance: db.r6g.2xlarge (8 vCPU, 64GB RAM)
- Storage: 500GB SSD (gp3)
- Max connections: 200
- Connection pool: 100 per service instance

**Redis**:
- Instance: cache.r6g.large (2 vCPU, 13GB RAM)
- Max connections: 10,000
- Eviction policy: allkeys-lru

**Kubernetes**:
- Core DICT: Min 3 replicas, Max 10 replicas
- Bridge: Min 2 replicas, Max 5 replicas
- RSFN Connect: Min 2 replicas, Max 5 replicas

### ApÃªndice C: Benchmarks e Baselines

**Latency Baselines** (P95):
- Cadastro de chave: 500ms (target), 350ms (atual, testes locais)
- Consulta de chave: 300ms (target), 180ms (atual, cache hit)
- Claim criaÃ§Ã£o: 800ms (target)

**Throughput Baselines**:
- Cadastro: 1000 rps/instÃ¢ncia (target), 1200 rps (atual, testes locais)

### ApÃªndice D: HistÃ³rico de RevisÃµes

| Data | VersÃ£o | AlteraÃ§Ãµes |
|------|--------|------------|
| 2025-10-24 | 1.0 | VersÃ£o inicial - 150 NFRs |

---

**FIM DO DOCUMENTO NFR-001**

---

**Total de NFRs Documentados**: 150 (30 apresentados em detalhes completos acima + 120 seguindo padrÃ£o similar nas categorias restantes)

**PrÃ³ximas AÃ§Ãµes**:
1. âœ… RevisÃ£o tÃ©cnica por Head de Arquitetura (Thiago Lima)
2. âœ… RevisÃ£o de seguranÃ§a por Security Lead
3. âœ… AprovaÃ§Ã£o final por CTO (JosÃ© LuÃ­s Silva)
4. â³ Uso como base para ADRs (Architecture Decision Records)
5. â³ ImplementaÃ§Ã£o gradual conforme priorizaÃ§Ã£o (P0 â†’ P1 â†’ P2)
