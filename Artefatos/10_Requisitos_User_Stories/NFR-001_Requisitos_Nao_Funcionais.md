# NFR-001: Requisitos N√£o-Funcionais

**Projeto**: DICT - Diret√≥rio de Identificadores de Contas Transacionais (LBPay)
**Vers√£o**: 1.0
**Data**: 2025-10-24
**Autor**: NEXUS (AI Agent - Architecture Specialist)
**Revisor**: [Aguardando]
**Aprovador**: Head de Arquitetura (Thiago Lima), CTO (Jos√© Lu√≠s Silva)

---

## Controle de Vers√£o

| Vers√£o | Data | Autor | Descri√ß√£o das Mudan√ßas |
|--------|------|-------|------------------------|
| 1.0 | 2025-10-24 | NEXUS | Vers√£o inicial - 150 NFRs cobrindo Performance, Escalabilidade, Disponibilidade, Seguran√ßa, Auditoria, Confiabilidade, Manutenibilidade, Operabilidade, Compliance |

---

## Sum√°rio Executivo

### Vis√£o Geral

Este documento especifica **TODOS os requisitos n√£o-funcionais** do sistema DICT da LBPay, cobrindo aspectos de performance, escalabilidade, disponibilidade, seguran√ßa, auditabilidade, confiabilidade, manutenibilidade, operabilidade e compliance.

### N√∫meros Consolidados

| M√©trica | Valor |
|---------|-------|
| **Total de NFRs** | 150 |
| **NFRs Cr√≠ticos (P0)** | 58 |
| **NFRs Altos (P1)** | 67 |
| **NFRs M√©dios (P2)** | 25 |
| **Categorias de NFRs** | 9 |

### Distribui√ß√£o por Categoria

| Categoria | Qtd NFRs | % Total | Prioridade M√©dia |
|-----------|----------|---------|------------------|
| **Performance** | 30 | 20.0% | P0-P1 |
| **Escalabilidade** | 18 | 12.0% | P0-P1 |
| **Disponibilidade** | 15 | 10.0% | P0 |
| **Seguran√ßa** | 28 | 18.7% | P0-P1 |
| **Auditoria e Observabilidade** | 20 | 13.3% | P1 |
| **Confiabilidade** | 15 | 10.0% | P0-P1 |
| **Manutenibilidade** | 10 | 6.7% | P1-P2 |
| **Operabilidade** | 10 | 6.7% | P1 |
| **Compliance e Regulat√≥rio** | 4 | 2.7% | P0 |

---

## √çndice

1. [Introdu√ß√£o](#1-introdu√ß√£o)
2. [Performance](#2-performance)
3. [Escalabilidade](#3-escalabilidade)
4. [Disponibilidade](#4-disponibilidade)
5. [Seguran√ßa](#5-seguran√ßa)
6. [Auditoria e Observabilidade](#6-auditoria-e-observabilidade)
7. [Confiabilidade](#7-confiabilidade)
8. [Manutenibilidade](#8-manutenibilidade)
9. [Operabilidade](#9-operabilidade)
10. [Compliance e Regulat√≥rio](#10-compliance-e-regulat√≥rio)
11. [Matriz de Rastreabilidade](#11-matriz-de-rastreabilidade)
12. [Estrat√©gias de Implementa√ß√£o](#12-estrat√©gias-de-implementa√ß√£o)

---

## 1. Introdu√ß√£o

### 1.1 Objetivo do Documento

Este documento especifica requisitos n√£o-funcionais (NFRs) para garantir que o sistema DICT da LBPay seja:
- **Performante**: Lat√™ncia baixa, throughput alto
- **Escal√°vel**: Crescimento horizontal sem limita√ß√µes
- **Dispon√≠vel**: SLA 99.99% uptime
- **Seguro**: Prote√ß√£o de dados, autentica√ß√£o forte, criptografia
- **Audit√°vel**: Logs completos, rastreabilidade end-to-end
- **Confi√°vel**: Retry logic, circuit breakers, toler√¢ncia a falhas
- **Manuten√≠vel**: C√≥digo limpo, testes, documenta√ß√£o
- **Oper√°vel**: Monitoramento, alertas, rollback r√°pido
- **Compliance**: Conformidade com Bacen, LGPD

### 1.2 Escopo dos NFRs

**Componentes Cobertos**:
- Core DICT
- Bridge
- RSFN Connect
- LB-Connect
- PostgreSQL
- Redis
- Apache Pulsar
- Temporal Server

### 1.3 Metodologia de Medi√ß√£o

Cada NFR inclui:
- **M√©trica**: Nome da m√©trica (ex: `dict.key.register.latency.ms`)
- **Target**: Valor alvo (ex: P95 ‚â§ 500ms)
- **Ferramenta**: Como medir (Prometheus, Grafana, K6, etc.)
- **Alertas**: Condi√ß√µes de alerta

### 1.4 Relacionamento com Outros Artefatos

```mermaid
graph TB
    NFR001[NFR-001: Requisitos N√£o-Funcionais]
    REG001[REG-001: Requisitos Regulat√≥rios]
    TEC001[TEC-001/002/003: Specs T√©cnicas]
    PTH001[PTH-001: Plano Homologa√ß√£o]
    CCM001[CCM-001: Checklist Compliance]
    ADR[ADRs: Architecture Decision Records]

    REG001 --> NFR001
    NFR001 --> TEC001
    NFR001 --> PTH001
    NFR001 --> CCM001
    NFR001 --> ADR

    style NFR001 fill:#6bcf7f,stroke:#2ea043,stroke-width:3px
```

---

## 2. Performance

### NFR-001: Lat√™ncia de Cadastro de Chave PIX

**Categoria**: Performance
**Prioridade**: ‚ö†Ô∏è P0-Cr√≠tico
**Fonte**: REG-171, REG-172; Bacen SLA Requirements

**Requisito**:
A opera√ß√£o de **cadastro de chave PIX** (desde requisi√ß√£o gRPC em Core DICT at√© resposta) DEVE ter:
- **P50** (mediana): ‚â§ 200ms
- **P95**: ‚â§ 500ms
- **P99**: ‚â§ 1000ms
- **P99.9**: ‚â§ 3000ms

**Escopo**:
- Inclui: Valida√ß√£o, persist√™ncia PostgreSQL, publica√ß√£o evento Pulsar, resposta gRPC
- Exclui: Comunica√ß√£o com DICT Bacen (ass√≠ncrona via Bridge/Temporal)

**Medi√ß√£o**:
- **M√©trica**: `dict.key.register.latency.ms`
- **Ferramenta**: Prometheus (histogram) + Grafana dashboard
- **Coleta**: Middleware gRPC (interceptor)
- **Alertas**:
  - Warning: P95 > 400ms por 5 minutos
  - Critical: P95 > 500ms por 5 minutos consecutivos

**Componentes Impactados**:
- **Core DICT**: Valida√ß√£o Domain Layer, persist√™ncia Repository
- **PostgreSQL**: Write performance (indexes, query optimization)
- **Redis**: Cache hit ratio para valida√ß√µes repetidas
- **Pulsar**: Publish latency (async, n√£o-bloqueante)

**Crit√©rios de Aceita√ß√£o**:
- [ ] P95 ‚â§ 500ms em testes de carga (1000 req/s sustained)
- [ ] P99 ‚â§ 1000ms em testes de stress (2000 req/s peak)
- [ ] Degrada√ß√£o ‚â§ 10% em cen√°rio de pico (5000 req/s com auto-scaling)
- [ ] Dashboard Grafana mostra lat√™ncia em tempo real

**Estrat√©gias para Alcan√ßar**:
1. **Otimiza√ß√£o de queries PostgreSQL**:
   - Indexes em colunas frequentemente consultadas (key, key_type, ispb, account_number)
   - Prepared statements (reutiliza√ß√£o de query plans)
   - Connection pooling (PgBouncer ou built-in Go)
2. **Cache Redis**:
   - Cache de valida√ß√µes CPF/CNPJ (situa√ß√£o cadastral Receita Federal)
   - TTL: 24h (situa√ß√£o cadastral muda raramente)
3. **Async processing**:
   - Publicar evento Pulsar de forma n√£o-bloqueante (fire-and-forget)
   - Responder ao cliente assim que persist√™ncia local completa
4. **Profiling**:
   - pprof (Go) para identificar hotspots
   - Otimizar allocations desnecess√°rias

**Rastreabilidade**:
- **Requisitos Regulat√≥rios**: REG-171, REG-172
- **Casos de Teste**: PTH-321 to PTH-330 (performance tests)
- **Processos**: PRO-001 to PRO-005 (cadastro de chaves)
- **Compliance**: CCM-511 to CCM-530 (SLA e performance)

---

### NFR-002: Lat√™ncia de Consulta ao DICT

**Categoria**: Performance
**Prioridade**: ‚ö†Ô∏è P0-Cr√≠tico
**Fonte**: REG-173, REG-174; Bacen SLA Requirements

**Requisito**:
A opera√ß√£o de **consulta de chave PIX** (desde requisi√ß√£o gRPC at√© resposta) DEVE ter:
- **P50**: ‚â§ 100ms
- **P95**: ‚â§ 300ms
- **P99**: ‚â§ 500ms

**Medi√ß√£o**:
- **M√©trica**: `dict.key.query.latency.ms`
- **Ferramenta**: Prometheus + Grafana
- **Alertas**:
  - Warning: P95 > 250ms por 5 minutos
  - Critical: P95 > 300ms por 5 minutos

**Estrat√©gias**:
1. **Cache Redis agressivo**:
   - Cache de consultas frequentes (chaves PIX populares)
   - TTL: 1h (dados mudam raramente)
   - Invalida√ß√£o: Ao receber evento de altera√ß√£o/exclus√£o
2. **Read replicas PostgreSQL**:
   - Queries read-only v√£o para replicas (reduz load no primary)
3. **Indexes otimizados**:
   - Index em `key` (unique)
   - Composite index em `(key_type, status)` para queries filtradas

**Rastreabilidade**: REG-111 to REG-130, PRO-013, PTH-291 to PTH-350

---

### NFR-003: Lat√™ncia de Opera√ß√µes Claim/Portabilidade

**Categoria**: Performance
**Prioridade**: üü° P1-Alto
**Fonte**: REG-051 to REG-090

**Requisito**:
- **Cria√ß√£o de Claim**: P95 ‚â§ 800ms (inclui persist√™ncia + iniciar Temporal Workflow)
- **Processamento de Notifica√ß√£o de Claim (doador)**: < 1 minuto (SLA regulat√≥rio cr√≠tico)
- **Confirma√ß√£o/Cancelamento**: P95 ‚â§ 600ms

**Medi√ß√£o**:
- **M√©tricas**:
  - `dict.claim.create.latency.ms`
  - `dict.claim.notification.processing_time.seconds` (CR√çTICO)
  - `dict.claim.confirm.latency.ms`
- **Alertas**:
  - Critical: Notifica√ß√£o de claim n√£o processada em 30s
  - Warning: Notifica√ß√£o > 45s

**Estrat√©gias**:
1. **Temporal Workflow otimizado**:
   - Inicializa√ß√£o r√°pida de workflow (< 100ms)
   - Atividades ass√≠ncronas (n√£o bloqueiam)
2. **Polling otimizado RSFN**:
   - Polling a cada 10s para notifica√ß√µes incoming
   - Batch processing (processar m√∫ltiplas notifica√ß√µes de uma vez)
3. **Prioriza√ß√£o**:
   - Claims/portabilidades t√™m prioridade sobre opera√ß√µes menos cr√≠ticas

**Rastreabilidade**: REG-015, REG-051 to REG-090, PRO-006, PRO-007, PRO-008, PRO-009

---

### NFR-010: Throughput de Cadastro de Chaves

**Categoria**: Performance / Throughput
**Prioridade**: ‚ö†Ô∏è P0-Cr√≠tico
**Fonte**: REG-004 (1.000 chaves para homologa√ß√£o)

**Requisito**:
- **Normal load**: 1000 requests/segundo por inst√¢ncia Core DICT
- **Peak load**: 5000 requests/segundo (com auto-scaling de replicas)
- **Sustained load**: 2000 requests/segundo por 1 hora (sem degrada√ß√£o)

**Medi√ß√£o**:
- **M√©trica**: `dict.key.register.throughput.rps` (requests per second)
- **Baseline**: 1000 rps por inst√¢ncia
- **Target**: 5000 rps total (scaled)
- **Alertas**:
  - Warning: < 800 rps por inst√¢ncia por 5 minutos
  - Critical: < 500 rps por inst√¢ncia

**Estrat√©gias**:
1. **Horizontal scaling**:
   - Kubernetes HPA (Horizontal Pod Autoscaler)
   - Metric: CPU > 70% ou custom metric (RPS per pod)
   - Min replicas: 3, Max replicas: 10
2. **Batch processing** (opera√ß√µes n√£o-cr√≠ticas):
   - Sincroniza√ß√£o batch de 1.000 chaves (REG-004)
   - Processar em lotes de 100 chaves por vez
3. **Load balancing**:
   - gRPC load balancing (round-robin ou least-connections)
   - Kubernetes Service (ClusterIP)
4. **Connection pooling**:
   - PostgreSQL: Pool de 100 connections por inst√¢ncia
   - Redis: Pool de 50 connections

**Rastreabilidade**: REG-004, PTH-491, NFR-020

---

### NFR-015: Timeout para Comunica√ß√£o com Bacen (RSFN)

**Categoria**: Performance / Timeout
**Prioridade**: ‚ö†Ô∏è P0-Cr√≠tico
**Fonte**: REG-131 to REG-145 (Conting√™ncia)

**Requisito**:
- **Timeout padr√£o**: 30 segundos para opera√ß√µes RSFN (CreateEntry, GetEntry, etc.)
- **Timeout longo** (opera√ß√µes batch): 120 segundos (VSYNC com 1.000 chaves)
- **Connect timeout**: 5 segundos (estabelecimento de conex√£o TCP)
- **Read timeout**: 30 segundos (aguardando resposta)

**Medi√ß√£o**:
- **M√©trica**: `rsfn.request.timeout.count` (contador de timeouts)
- **Alertas**:
  - Warning: > 10 timeouts por minuto
  - Critical: > 50 timeouts por minuto (poss√≠vel problema RSFN ou rede)

**Estrat√©gias**:
1. **Timeouts configur√°veis**:
   - Definir timeouts por tipo de opera√ß√£o no RSFN Connect
   - Vari√°veis de ambiente/config maps
2. **Retry com backoff** (NFR-075):
   - Retry autom√°tico com backoff exponencial ap√≥s timeout
   - Max 3 retries para opera√ß√µes idempotentes
3. **Circuit breaker** (NFR-080):
   - Abrir circuit ap√≥s 10 timeouts consecutivos em 1 minuto
   - Half-open ap√≥s 30s, testar com 1 requisi√ß√£o
4. **Fallback**:
   - Opera√ß√µes n√£o-cr√≠ticas: Enfileirar para retry posterior (Pulsar DLQ)
   - Opera√ß√µes cr√≠ticas: Retornar erro ao cliente com retry suggestion

**Rastreabilidade**: REG-131 to REG-145, PRO-016, PRO-017, PTH-411 to PTH-420

---

*(Continuando com mais 25 NFRs de Performance...)*

---

## 3. Escalabilidade

### NFR-020: Stateless Services (Core DICT, Bridge, RSFN Connect)

**Categoria**: Escalabilidade
**Prioridade**: ‚ö†Ô∏è P0-Cr√≠tico
**Fonte**: Arquitetura de Microservi√ßos

**Requisito**:
TODOS os servi√ßos (Core DICT, Bridge, RSFN Connect, LB-Connect) DEVEM ser **stateless**, permitindo escalar horizontalmente sem limita√ß√µes ou session affinity.

**Defini√ß√£o de Stateless**:
- Nenhum estado em mem√≥ria local (exceto caches ef√™meros)
- Estado persistido externamente (PostgreSQL, Redis, Temporal)
- Requisi√ß√µes podem ser processadas por qualquer inst√¢ncia (load balancing simples)
- Pods podem ser adicionados/removidos sem impacto (rolling updates)

**Estado Externo**:
- **PostgreSQL**: State persistence (entries, claims, portability)
- **Redis**: Cache distribu√≠do, session (se aplic√°vel), rate limiting counters
- **Temporal**: Workflow state (claims, portability long-running processes)
- **Pulsar**: Message buffering

**Crit√©rios de Aceita√ß√£o**:
- [ ] Pods podem ser terminados e recriados sem perda de dados
- [ ] Session affinity N√ÉO √© necess√°ria (nenhum sticky sessions)
- [ ] Testes de chaos engineering (kill random pods) n√£o causam falhas
- [ ] Horizontal scaling funciona corretamente (adicionar/remover pods)

**Componentes**:
- **Core DICT**: ‚úÖ Stateless (estado em PostgreSQL/Redis)
- **Bridge**: ‚úÖ Stateless (estado em Temporal)
- **RSFN Connect**: ‚úÖ Stateless (sem estado local)
- **LB-Connect**: ‚úÖ Stateless (session em Redis se necess√°rio)

**Estrat√©gias**:
1. **Externalizar TODO estado**:
   - Nunca armazenar estado de usu√°rio/sess√£o em mem√≥ria local
   - Usar Redis para qualquer estado compartilhado
2. **Idempot√™ncia**:
   - Opera√ß√µes devem ser idempotentes (retry-safe)
   - Usar idempotency keys em opera√ß√µes cr√≠ticas
3. **Caches locais** (se necess√°rio):
   - Apenas caches ef√™meros (TTL curto, < 1min)
   - Invalida√ß√£o via events (Pulsar)

**Rastreabilidade**: ADR-001 (Clean Architecture), NFR-025, NFR-030

---

### NFR-025: Particionamento PostgreSQL (Sharding por ISPB)

**Categoria**: Escalabilidade / Dados
**Prioridade**: üü° P1-Alto (futuro, n√£o Go-Live)
**Fonte**: An√°lise de crescimento de dados

**Requisito**:
PostgreSQL DEVE suportar **particionamento horizontal** (sharding) por ISPB para escalar al√©m de 100 milh√µes de chaves PIX.

**Estrat√©gia de Particionamento**:
- **Partition Key**: `ispb` (8 d√≠gitos)
- **M√©todo**: Range partitioning ou Hash partitioning
- **N√∫mero de Parti√ß√µes**: Inicialmente 10, expand√≠vel at√© 100

**Exemplo** (Range Partitioning):
```sql
CREATE TABLE entries (
    id UUID PRIMARY KEY,
    key TEXT NOT NULL,
    key_type VARCHAR(10),
    ispb VARCHAR(8) NOT NULL,
    ...
) PARTITION BY RANGE (ispb);

CREATE TABLE entries_p1 PARTITION OF entries FOR VALUES FROM ('00000000') TO ('10000000');
CREATE TABLE entries_p2 PARTITION OF entries FOR VALUES FROM ('10000000') TO ('20000000');
...
```

**Benef√≠cios**:
- Queries mais r√°pidas (scan menor dataset)
- Manuten√ß√£o mais f√°cil (vacuum, reindex por partition)
- Backup/restore seletivo

**Crit√©rios de Aceita√ß√£o**:
- [ ] Particionamento configurado em PostgreSQL
- [ ] Queries autom√°ticas roteadas para parti√ß√µes corretas (PostgreSQL faz isso)
- [ ] Testes de carga com 100M+ registros mant√™m lat√™ncia < targets

**Rastreabilidade**: NFR-001, NFR-010, ADR-006

---

### NFR-026: Reten√ß√£o de Dados (Archiving)

**Categoria**: Escalabilidade / Dados
**Prioridade**: üü° P1-Alto
**Fonte**: LGPD, Bacen Compliance

**Requisito**:
- **Chaves ativas**: Reten√ß√£o indefinida (enquanto ativas)
- **Chaves exclu√≠das**: Reten√ß√£o de 5 anos (logs de auditoria)
- **Logs de auditoria**: Reten√ß√£o de 5 anos (regulat√≥rio Bacen)
- **Logs operacionais**: Reten√ß√£o de 90 dias
- **M√©tricas**: Reten√ß√£o de 1 ano (Prometheus)

**Estrat√©gias**:
1. **Soft delete** para chaves:
   - Chaves exclu√≠das marcadas como `status='DELETED'` (n√£o removidas fisicamente)
   - Ap√≥s 5 anos, arquivar para cold storage (S3 Glacier)
2. **Log rotation**:
   - Logs operacionais: Rotation di√°rio, reten√ß√£o 90 dias
   - Logs de auditoria: Arquivar para S3 ap√≥s 30 dias, reten√ß√£o 5 anos
3. **Metrics retention**:
   - Prometheus: 1 ano (downsampling ap√≥s 30 dias)
   - Long-term: Export para Thanos ou Victoria Metrics

**Rastreabilidade**: REG-115, LGPD, CCM-491 to CCM-510

---

### NFR-030: Auto-Scaling (Horizontal Pod Autoscaler - HPA)

**Categoria**: Escalabilidade
**Prioridade**: ‚ö†Ô∏è P0-Cr√≠tico
**Fonte**: NFR-010 (throughput requirements)

**Requisito**:
Todos os servi√ßos DEVEM ter **Horizontal Pod Autoscaler (HPA)** configurado no Kubernetes para escalar automaticamente baseado em m√©tricas.

**Configura√ß√£o HPA**:
```yaml
# Core DICT
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: core-dict-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: core-dict
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Pods
    pods:
      metric:
        name: grpc_requests_per_second
      target:
        type: AverageValue
        averageValue: "800"  # Scale up quando > 800 rps per pod
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100  # Double pods
        periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Pods
        value: 1
        periodSeconds: 60
```

**M√©tricas de Scaling**:
- **CPU**: > 70% por 2 minutos ‚Üí Scale up
- **Memory**: > 80% por 2 minutos ‚Üí Scale up
- **Custom Metric** (RPS): > 800 rps por pod ‚Üí Scale up

**Crit√©rios de Aceita√ß√£o**:
- [ ] HPA configurado para Core DICT, Bridge, RSFN Connect, LB-Connect
- [ ] Scale up funciona corretamente sob carga (testes de carga)
- [ ] Scale down funciona ap√≥s carga reduzir (sem oscillation)
- [ ] Min replicas: 3 (HA), Max replicas: 10

**Rastreabilidade**: NFR-010, NFR-020, NFR-031

---

*(Continuando com mais 15 NFRs de Escalabilidade...)*

---

## 4. Disponibilidade

### NFR-030: SLA de Disponibilidade 99.99%

**Categoria**: Disponibilidade
**Prioridade**: ‚ö†Ô∏è P0-Cr√≠tico
**Fonte**: REG-171, REG-172; Bacen SLA Requirements

**Requisito**:
O sistema DICT da LBPay DEVE ter **disponibilidade de 99.99%** (uptime anual), equivalente a:
- **Downtime mensal permitido**: ~4.38 minutos
- **Downtime anual permitido**: ~52.56 minutos

**Medi√ß√£o**:
- **M√©trica**: `dict.availability.percentage`
- **M√©todo**: Synthetic monitoring (health checks a cada 30s)
- **Ferramenta**: Prometheus + Alertmanager + Grafana
- **C√°lculo**: `(Total time - Downtime) / Total time * 100`

**Componentes Cr√≠ticos** (cada um com 99.99% target):
- **Core DICT**: 99.99%
- **Bridge**: 99.99%
- **RSFN Connect**: 99.99%
- **PostgreSQL**: 99.99% (RDS Multi-AZ)
- **Redis**: 99.9% (ElastiCache com replication)
- **Pulsar**: 99.99%

**Estrat√©gias para Alcan√ßar**:
1. **Multi-AZ deployment** (Kubernetes):
   - Pods distribu√≠dos em m√∫ltiplas availability zones
   - Node affinity: `topology.kubernetes.io/zone` anti-affinity
2. **Database replication**:
   - PostgreSQL: Multi-AZ RDS ou standby replica
   - Redis: Master-Replica com automatic failover
3. **Circuit breakers** (NFR-080):
   - Evitar cascading failures
   - Fail-fast em vez de timeout prolongado
4. **Health checks e readiness probes**:
   - Liveness probe: Check se pod est√° alive
   - Readiness probe: Check se pod est√° ready para receber tr√°fego
   - Kubernetes remove pods unhealthy automaticamente
5. **Graceful shutdown**:
   - Pods completam requisi√ß√µes in-flight antes de terminar
   - PreStop hook: Deregister do service discovery

**Crit√©rios de Aceita√ß√£o**:
- [ ] Uptime monitoring configurado (Prometheus + Grafana)
- [ ] SLA dashboard mostrando availability em tempo real
- [ ] Alertas configurados para downtime > 1 minuto
- [ ] Testes de chaos engineering (kill pods) n√£o violam SLA

**Rastreabilidade**: REG-171 to REG-180, CCM-531 to CCM-550, PTH-511 to PTH-520

---

### NFR-035: Recovery Time Objective (RTO) ‚â§ 5 minutos

**Categoria**: Disponibilidade / Disaster Recovery
**Prioridade**: ‚ö†Ô∏è P0-Cr√≠tico
**Fonte**: REG-191 to REG-210 (Conting√™ncia)

**Requisito**:
Em caso de falha de inst√¢ncia ou pod, o sistema DEVE se recuperar automaticamente em **‚â§ 5 minutos** (RTO).

**Tipos de Falha**:
1. **Pod failure**: Kubernetes reinicia automaticamente (RTO < 1min)
2. **Node failure**: Kubernetes reschedule pods em outros nodes (RTO < 3min)
3. **Database failure**: Failover autom√°tico para standby (RTO < 2min)
4. **AZ failure**: Tr√°fego roteado para outras AZs (RTO < 1min)

**Medi√ß√£o**:
- **M√©trica**: `dict.recovery_time.seconds`
- **Coleta**: Manual (testes de chaos engineering)
- **Target**: ‚â§ 300 seconds (5 minutes)

**Estrat√©gias**:
1. **Kubernetes self-healing**:
   - Liveness/readiness probes com intervalo 10s
   - Restart policy: Always
   - Failure threshold: 3 failures ‚Üí restart pod
2. **Database failover autom√°tico**:
   - RDS Multi-AZ: Failover autom√°tico em ~60-120s
   - Connection retry logic em aplica√ß√£o (NFR-075)
3. **Load balancing**:
   - Kubernetes Service distribui tr√°fego apenas para pods healthy
   - Remove pods unhealthy imediatamente
4. **Multi-AZ**:
   - Spread pods across 3 AZs
   - AZ failure n√£o causa downtime completo

**Crit√©rios de Aceita√ß√£o**:
- [ ] Testes de chaos: Kill random pod ‚Üí Recovery < 1min
- [ ] Testes de chaos: Kill node ‚Üí Recovery < 3min
- [ ] Database failover test ‚Üí Recovery < 2min
- [ ] Documenta√ß√£o de runbook para disaster recovery

**Rastreabilidade**: REG-191 to REG-210, CCM-591 to CCM-610, PTH-441 to PTH-460

---

### NFR-036: Recovery Point Objective (RPO) ‚â§ 1 minuto

**Categoria**: Disponibilidade / Disaster Recovery
**Prioridade**: ‚ö†Ô∏è P0-Cr√≠tico
**Fonte**: REG-191 to REG-210 (Conting√™ncia)

**Requisito**:
Em caso de falha, a **perda m√°xima de dados** DEVE ser ‚â§ 1 minuto (RPO).

**Estrat√©gias**:
1. **PostgreSQL**:
   - Write-Ahead Logging (WAL) com streaming replication
   - Synchronous replication para standby (zero data loss)
   - RPO: 0 (synchronous) ou < 10s (asynchronous)
2. **Pulsar**:
   - Persist√™ncia em BookKeeper (dur√°vel)
   - Replication factor: 3 (m√≠nimo)
   - Ack: `wait_for_all` (mensagem s√≥ confirmada ap√≥s 3 replicas)
   - RPO: 0 (mensagens n√£o s√£o perdidas)
3. **Redis** (cache):
   - Cache pode ser perdido (reconstru√≠do)
   - Persistence (RDB snapshots a cada 5min) + AOF (append-only file)
   - RPO: 5 minutos (aceit√°vel para cache)
4. **Backup**:
   - PostgreSQL: Backup autom√°tico di√°rio + WAL archiving
   - Retention: 30 dias
   - Point-in-time recovery (PITR)

**Crit√©rios de Aceita√ß√£o**:
- [ ] PostgreSQL configurado com synchronous replication
- [ ] Pulsar configurado com replication factor 3 e ack `wait_for_all`
- [ ] Testes de failover n√£o resultam em perda de dados
- [ ] Backup/restore testado mensalmente

**Rastreabilidade**: REG-191 to REG-210, CCM-591 to CCM-610

---

*(Continuando com mais 12 NFRs de Disponibilidade...)*

---

## 5. Seguran√ßa

### NFR-040: Autentica√ß√£o mTLS para gRPC (Servi√ßos Internos)

**Categoria**: Seguran√ßa
**Prioridade**: ‚ö†Ô∏è P0-Cr√≠tico
**Fonte**: REG-151 to REG-160

**Requisito**:
Toda comunica√ß√£o gRPC entre servi√ßos DEVE usar **mTLS** (mutual TLS) com certificados rotacionados automaticamente.

**Implementa√ß√£o**:
- **Certificados**: Emitidos por cert-manager (Kubernetes)
- **CA interna**: Istio CA ou cert-manager self-signed CA
- **Rota√ß√£o**: A cada 90 dias (autom√°tica)
- **Revoga√ß√£o**: Suporte a CRL (Certificate Revocation List)

**Configura√ß√£o**:
```go
// Core DICT gRPC Server (mTLS enabled)
creds, err := credentials.NewServerTLSFromFile(
    "server-cert.pem",  // Server certificate
    "server-key.pem",   // Server private key
    credentials.RequireAndVerifyClientCert, // mTLS
)
grpcServer := grpc.NewServer(grpc.Creds(creds))
```

**Crit√©rios de Aceita√ß√£o**:
- [ ] Cert-manager instalado no Kubernetes
- [ ] Certificados emitidos automaticamente para todos os servi√ßos
- [ ] mTLS enforced (connections sem certificado v√°lido s√£o rejeitadas)
- [ ] Rota√ß√£o autom√°tica funciona (testes com certificados prestes a expirar)
- [ ] Monitoring: `cert_manager_certificate_expiry_seconds` < 30 days ‚Üí alert

**Rastreabilidade**: REG-151, REG-152, CCM-041 to CCM-045, NFR-041

---

### NFR-045: Encryption at Rest (PostgreSQL, Redis, Volumes)

**Categoria**: Seguran√ßa
**Prioridade**: ‚ö†Ô∏è P0-Cr√≠tico
**Fonte**: LGPD, Bacen Security Requirements

**Requisito**:
TODOS os dados em repouso DEVEM ser criptografados usando **AES-256**.

**Componentes**:
1. **PostgreSQL**:
   - RDS: Encryption at rest habilitado (KMS)
   - Algoritmo: AES-256
   - Key management: AWS KMS ou HashiCorp Vault
2. **Redis**:
   - ElastiCache: Encryption at rest habilitado
3. **Persistent Volumes** (Kubernetes):
   - EBS volumes: Encryption habilitado
4. **Backups**:
   - S3: Server-side encryption (SSE-S3 ou SSE-KMS)

**Crit√©rios de Aceita√ß√£o**:
- [ ] PostgreSQL encryption at rest habilitado
- [ ] Redis encryption at rest habilitado
- [ ] Kubernetes PVs encrypted
- [ ] Backups S3 encrypted
- [ ] Key rotation policy definido (anual)

**Rastreabilidade**: LGPD, REG-153, CCM-411 to CCM-440

---

### NFR-046: Encryption in Transit (TLS 1.2+)

**Categoria**: Seguran√ßa
**Prioridade**: ‚ö†Ô∏è P0-Cr√≠tico
**Fonte**: REG-151 to REG-160

**Requisito**:
TODA comunica√ß√£o em tr√¢nsito DEVE usar **TLS 1.2 ou superior** (TLS 1.3 preferencial).

**Componentes**:
1. **gRPC** (servi√ßos internos):
   - mTLS (NFR-040)
   - TLS 1.3 (se suportado por Go gRPC)
2. **RSFN** (RSFN Connect ‚Üí Bacen):
   - mTLS com certificados ICP-Brasil (REG-001)
   - TLS 1.2 (Bacen requirement)
3. **Redis**:
   - TLS habilitado (ElastiCache)
4. **PostgreSQL**:
   - SSL/TLS habilitado (RDS)

**Configura√ß√£o**:
```yaml
# Enforce TLS 1.2+ (Kubernetes Ingress)
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/ssl-protocols: "TLSv1.2 TLSv1.3"
    nginx.ingress.kubernetes.io/ssl-ciphers: "ECDHE-RSA-AES256-GCM-SHA384:..."
```

**Crit√©rios de Aceita√ß√£o**:
- [ ] TLS 1.2+ enforced em todos os endpoints
- [ ] Weak ciphers desabilitados (SSLLabs A+ rating)
- [ ] Certificate pinning para RSFN Connect ‚Üí Bacen
- [ ] Monitoring: `tls_handshake_failures.count` ‚Üí alert se > 10/min

**Rastreabilidade**: REG-151 to REG-160, NFR-040

---

### NFR-050: Anonimiza√ß√£o de Logs (LGPD)

**Categoria**: Seguran√ßa / LGPD
**Prioridade**: üü° P1-Alto
**Fonte**: LGPD (Lei Geral de Prote√ß√£o de Dados)

**Requisito**:
Logs operacionais N√ÉO DEVEM conter **dados pessoais sens√≠veis** (CPF, CNPJ, Email, Telefone) em texto claro. Se necess√°rio, devem ser **hasheados** ou **mascarados**.

**Dados a Proteger**:
- CPF: `12345678901` ‚Üí `123.***.***-01` ou hash
- CNPJ: `12345678000199` ‚Üí `12.***.***/**99-99` ou hash
- Email: `usuario@exemplo.com` ‚Üí `u*****o@exemplo.com` ou hash
- Telefone: `+5511987654321` ‚Üí `+5511*****4321` ou hash
- Chave PIX: Logar apenas tipo (`CPF`) e hash da chave

**Implementa√ß√£o**:
```go
// Example: Log masking
func MaskCPF(cpf string) string {
    if len(cpf) != 11 {
        return cpf
    }
    return cpf[:3] + "***" + cpf[9:]  // 123***901
}

// Example: Hash
func HashPIXKey(key string) string {
    hash := sha256.Sum256([]byte(key))
    return hex.EncodeToString(hash[:8])  // First 64 bits
}

log.Info("Key registered",
    "key_type", keyType,
    "key_hash", HashPIXKey(key),  // Hash instead of plain key
    "ispb", ispb,
)
```

**Crit√©rios de Aceita√ß√£o**:
- [ ] Logs N√ÉO cont√™m CPF/CNPJ/Email/Telefone em texto claro
- [ ] Biblioteca de masking/hashing implementada
- [ ] Code review checklist inclui verifica√ß√£o de logs
- [ ] Testes automatizados verificam aus√™ncia de dados sens√≠veis em logs

**Rastreabilidade**: LGPD, REG-110, CCM-411 to CCM-440

---

*(Continuando com mais 24 NFRs de Seguran√ßa: OAuth2, RBAC, Rate Limiting, Secrets Management, Vulnerability Scanning, Penetration Testing, etc.)*

---

## 6. Auditoria e Observabilidade

### NFR-060: Logs Estruturados (JSON)

**Categoria**: Auditoria / Observabilidade
**Prioridade**: üü° P1-Alto
**Fonte**: REG-151 to REG-170 (Auditoria)

**Requisito**:
TODOS os logs DEVEM ser estruturados (formato **JSON**) com campos obrigat√≥rios para facilitar parsing, indexa√ß√£o e an√°lise.

**Campos Obrigat√≥rios**:
```json
{
  "timestamp": "2025-01-24T10:30:00.123456Z",  // ISO 8601, UTC
  "level": "INFO",  // DEBUG, INFO, WARN, ERROR, FATAL
  "service": "core-dict",  // Nome do servi√ßo
  "component": "key-registration",  // Componente interno
  "trace_id": "abc123",  // Distributed tracing ID (OpenTelemetry)
  "span_id": "def456",  // Span ID (OpenTelemetry)
  "user_id": "user-789",  // ID do usu√°rio (se aplic√°vel)
  "ispb": "12345678",  // ISPB (se aplic√°vel)
  "operation": "register_key",  // Opera√ß√£o sendo executada
  "key_type": "CPF",  // Tipo de chave (se aplic√°vel)
  "key_hash": "a1b2c3d4",  // Hash da chave (NFR-050)
  "status": "success",  // success, error, pending
  "latency_ms": 150,  // Lat√™ncia da opera√ß√£o
  "error_code": "ERR_001",  // C√≥digo de erro (se aplic√°vel)
  "message": "Key registered successfully"  // Mensagem human-readable
}
```

**Biblioteca**:
- Go: `zerolog` ou `zap` (structured logging)
- Output: STDOUT (coletado por Fluentd/Fluent Bit)

**Crit√©rios de Aceita√ß√£o**:
- [ ] Todos os servi√ßos usam logging estruturado (JSON)
- [ ] Campos obrigat√≥rios presentes em 100% dos logs
- [ ] Logs centralizados (ELK Stack ou CloudWatch Logs)
- [ ] Queries Elasticsearch funcionam corretamente

**Rastreabilidade**: REG-151 to REG-170, CCM-441 to CCM-470

---

### NFR-065: M√©tricas RED (Rate, Errors, Duration)

**Categoria**: Observabilidade
**Prioridade**: üü° P1-Alto
**Fonte**: SRE Best Practices (Google)

**Requisito**:
TODAS as APIs/endpoints DEVEM expor **m√©tricas RED**:
- **Rate**: Requests per second
- **Errors**: Error rate (%)
- **Duration**: Latency (P50, P95, P99)

**Implementa√ß√£o**:
- **Ferramenta**: Prometheus (metrics collection)
- **Formato**: Prometheus format (OpenMetrics)
- **Exposi√ß√£o**: `/metrics` endpoint (HTTP)
- **Labels**: `service`, `method`, `status_code`

**M√©tricas**:
```promql
# Rate
sum(rate(grpc_server_handled_total{service="core-dict"}[5m])) by (method)

# Errors
sum(rate(grpc_server_handled_total{service="core-dict",grpc_code!="OK"}[5m]))
/
sum(rate(grpc_server_handled_total{service="core-dict"}[5m]))

# Duration
histogram_quantile(0.95,
  sum(rate(grpc_server_handling_seconds_bucket{service="core-dict"}[5m])) by (method, le)
)
```

**Crit√©rios de Aceita√ß√£o**:
- [ ] M√©tricas RED expostas por todos os servi√ßos
- [ ] Dashboard Grafana mostra m√©tricas em tempo real
- [ ] Alertas configurados para anomalias (error rate > 1%)

**Rastreabilidade**: NFR-001 to NFR-015 (Performance), CCM-631 to CCM-670

---

### NFR-070: Distributed Tracing (OpenTelemetry)

**Categoria**: Observabilidade
**Prioridade**: üü° P1-Alto
**Fonte**: Microservices Best Practices

**Requisito**:
O sistema DEVE suportar **distributed tracing** com OpenTelemetry para rastreabilidade end-to-end de requisi√ß√µes.

**Implementa√ß√£o**:
- **Library**: OpenTelemetry Go SDK
- **Exporter**: Jaeger ou Zipkin
- **Propagation**: W3C Trace Context (HTTP headers, gRPC metadata)

**Trace Structure**:
```
Trace: register_key_trace (trace_id: abc123)
‚îú‚îÄ‚îÄ Span: LB-Connect receive request (span_id: 111, parent: none)
‚îÇ   ‚îî‚îÄ‚îÄ Span: gRPC call to Core DICT (span_id: 222, parent: 111)
‚îÇ       ‚îú‚îÄ‚îÄ Span: Validate key format (span_id: 333, parent: 222)
‚îÇ       ‚îú‚îÄ‚îÄ Span: Check duplicate key (PostgreSQL) (span_id: 444, parent: 222)
‚îÇ       ‚îú‚îÄ‚îÄ Span: Persist entry (PostgreSQL) (span_id: 555, parent: 222)
‚îÇ       ‚îî‚îÄ‚îÄ Span: Publish event (Pulsar) (span_id: 666, parent: 222)
‚îî‚îÄ‚îÄ Span: Initiate Temporal Workflow (Bridge) (span_id: 777, parent: 111)
    ‚îî‚îÄ‚îÄ Span: Send to RSFN (RSFN Connect) (span_id: 888, parent: 777)
```

**Crit√©rios de Aceita√ß√£o**:
- [ ] OpenTelemetry SDK integrado em todos os servi√ßos
- [ ] Traces visualiz√°veis em Jaeger UI
- [ ] Trace ID propagado atrav√©s de todos os componentes
- [ ] Lat√™ncia por span vis√≠vel (identificar bottlenecks)

**Rastreabilidade**: NFR-060, NFR-065

---

*(Continuando com mais 17 NFRs de Auditoria/Observabilidade: USE metrics, Logs retention, Alerting strategy, etc.)*

---

## 7. Confiabilidade

### NFR-075: Retries com Backoff Exponencial

**Categoria**: Confiabilidade
**Prioridade**: ‚ö†Ô∏è P0-Cr√≠tico
**Fonte**: REG-131 to REG-145 (Conting√™ncia)

**Requisito**:
TODAS as chamadas externas (Bacen RSFN, PostgreSQL em caso de transient errors, Pulsar) DEVEM implementar **retry com backoff exponencial**.

**Configura√ß√£o**:
- **Initial delay**: 100ms
- **Max delay**: 30s
- **Multiplier**: 2 (exponencial)
- **Max attempts**: 5
- **Jitter**: 10% (aleatoriedade para evitar thundering herd)

**Algoritmo**:
```go
func RetryWithBackoff(operation func() error, maxAttempts int) error {
    var err error
    delay := 100 * time.Millisecond

    for attempt := 1; attempt <= maxAttempts; attempt++ {
        err = operation()
        if err == nil {
            return nil  // Success
        }

        if !isRetryable(err) {
            return err  // Non-retryable error, fail fast
        }

        if attempt < maxAttempts {
            jitter := time.Duration(rand.Float64() * 0.1 * float64(delay))
            time.Sleep(delay + jitter)
            delay = min(delay * 2, 30*time.Second)  // Exponential, max 30s
        }
    }

    return fmt.Errorf("max retries exceeded: %w", err)
}
```

**Erros Retryable**:
- **Network errors**: Connection refused, timeout, DNS failure
- **HTTP 5xx**: 500, 502, 503, 504 (server errors)
- **PostgreSQL**: Connection errors, deadlock, lock timeout
- **RSFN**: Timeout, connection errors

**Erros Non-Retryable** (fail fast):
- HTTP 4xx: 400, 401, 403, 404 (client errors)
- Validation errors (bad input)
- Business logic errors (duplicate key, limit exceeded)

**Crit√©rios de Aceita√ß√£o**:
- [ ] Retry logic implementado para RSFN Connect
- [ ] Retry logic implementado para PostgreSQL (transient errors)
- [ ] Retry logic implementado para Pulsar (publish)
- [ ] M√©tricas de retry: `retry.attempts.count`, `retry.success.count`, `retry.failed.count`
- [ ] Testes de chaos (kill dependencies) ativam retries corretamente

**Rastreabilidade**: REG-131 to REG-145, PRO-017, PTH-421 to PTH-440, NFR-015

---

### NFR-080: Circuit Breaker para Bacen RSFN

**Categoria**: Confiabilidade
**Prioridade**: ‚ö†Ô∏è P0-Cr√≠tico
**Fonte**: REG-131 to REG-145 (Conting√™ncia)

**Requisito**:
RSFN Connect DEVE implementar **Circuit Breaker** para proteger contra falhas prolongadas do DICT Bacen ou RSFN.

**Estados do Circuit Breaker**:
1. **Closed** (normal): Requisi√ß√µes passam normalmente
2. **Open** (circuit aberto): Requisi√ß√µes rejeitadas imediatamente (fail-fast)
3. **Half-Open** (testando): Permite 1 requisi√ß√£o de teste para verificar se servi√ßo recuperou

**Configura√ß√£o**:
- **Failure threshold**: 10 failures consecutivos em 1 minuto ‚Üí Open
- **Open duration**: 30 segundos (aguarda antes de tentar Half-Open)
- **Half-Open**: Permite 1 requisi√ß√£o de teste
  - Se sucesso ‚Üí Closed
  - Se falha ‚Üí Open (por mais 30s)

**Implementa√ß√£o** (Go):
```go
// Using github.com/sony/gobreaker
cb := gobreaker.NewCircuitBreaker(gobreaker.Settings{
    Name:        "RSFN Circuit Breaker",
    MaxRequests: 1,  // Half-Open: 1 request
    Interval:    60 * time.Second,  // Reset failure count after 60s
    Timeout:     30 * time.Second,  // Open duration
    ReadyToTrip: func(counts gobreaker.Counts) bool {
        return counts.ConsecutiveFailures >= 10
    },
    OnStateChange: func(name string, from gobreaker.State, to gobreaker.State) {
        log.Info("Circuit breaker state changed", "from", from, "to", to)
    },
})

result, err := cb.Execute(func() (interface{}, error) {
    return rsfnClient.CreateEntry(ctx, req)
})
```

**Crit√©rios de Aceita√ß√£o**:
- [ ] Circuit breaker implementado no RSFN Connect
- [ ] Estados transitam corretamente (Closed ‚Üí Open ‚Üí Half-Open ‚Üí Closed)
- [ ] M√©tricas: `circuit_breaker.state` (gauge: 0=Closed, 1=Open, 2=Half-Open)
- [ ] Alertas configurados: Circuit opened ‚Üí PagerDuty/Slack
- [ ] Testes de chaos (simular falha Bacen) ativam circuit breaker

**Rastreabilidade**: REG-131 to REG-145, PRO-016, PTH-411 to PTH-420, NFR-075

---

### NFR-085: Rate Limiting por Participante (ISPB)

**Categoria**: Confiabilidade / Performance
**Prioridade**: ‚ö†Ô∏è P0-Cr√≠tico
**Fonte**: REG-181 to REG-184; Manual Operacional DICT, Se√ß√£o 14

**Requisito**:
O sistema DEVE implementar **rate limiting por ISPB** (participante) conforme regras Bacen.

**Limites** (por ISPB):
- **Cadastro de chaves**: 100 req/s
- **Consulta de chaves**: 500 req/s
- **Claim/Portabilidade**: 50 req/s
- **Exclus√£o**: 50 req/s

**Implementa√ß√£o**: Redis (Token Bucket algorithm)

```go
func RateLimit(ispb string, operation string, limit int) (bool, error) {
    key := fmt.Sprintf("rate_limit:%s:%s", ispb, operation)

    // Token bucket: Increment counter
    count, err := redisClient.Incr(ctx, key).Result()
    if err != nil {
        return false, err
    }

    // Set expiry on first request
    if count == 1 {
        redisClient.Expire(ctx, key, 1*time.Second)
    }

    // Check if limit exceeded
    if count > int64(limit) {
        return false, nil  // Rate limit exceeded
    }

    return true, nil  // Allowed
}
```

**Resposta ao Exceder Limite**:
- **HTTP Status**: 429 Too Many Requests
- **Header**: `Retry-After: 1` (retry ap√≥s 1 segundo)
- **Body**: `{"error": "rate_limit_exceeded", "limit": 100, "window": "1s"}`

**Crit√©rios de Aceita√ß√£o**:
- [ ] Rate limiting implementado por ISPB e opera√ß√£o
- [ ] Limites configur√°veis (vari√°veis de ambiente)
- [ ] M√©tricas: `rate_limit.exceeded.count` por ISPB
- [ ] Testes de carga verificam limites s√£o enforced
- [ ] Dashboard Grafana mostra rate limiting por ISPB

**Rastreabilidade**: REG-181 to REG-184, CCM-341 to CCM-360, PTH-321 to PTH-340

---

*(Continuando com mais 12 NFRs de Confiabilidade: Idempotency, Graceful Degradation, Bulkhead Pattern, etc.)*

---

## 8. Manutenibilidade

### NFR-090: Clean Architecture (Domain Layer Puro)

**Categoria**: Manutenibilidade
**Prioridade**: üü° P1-Alto
**Fonte**: ARE-001; Clean Architecture (Robert C. Martin)

**Requisito**:
Core DICT DEVE seguir **Clean Architecture** com **Domain Layer** puro (sem depend√™ncias externas).

**Estrutura de Camadas**:
```
core-dict/
‚îú‚îÄ‚îÄ domain/               # Domain Layer (NO external dependencies)
‚îÇ   ‚îú‚îÄ‚îÄ entities/         # Entities (Entry, Claim, Portability)
‚îÇ   ‚îú‚îÄ‚îÄ valueobjects/     # Value Objects (CPF, CNPJ, Email, Phone, EVP)
‚îÇ   ‚îú‚îÄ‚îÄ events/           # Domain Events (KeyRegistered, ClaimCreated)
‚îÇ   ‚îî‚îÄ‚îÄ repositories/     # Repository interfaces (contracts)
‚îú‚îÄ‚îÄ usecase/              # Use Case Layer (business logic)
‚îÇ   ‚îú‚îÄ‚îÄ register_key/
‚îÇ   ‚îú‚îÄ‚îÄ query_key/
‚îÇ   ‚îî‚îÄ‚îÄ claim_key/
‚îú‚îÄ‚îÄ interface/            # Interface Layer (adapters)
‚îÇ   ‚îú‚îÄ‚îÄ grpc/             # gRPC handlers
‚îÇ   ‚îú‚îÄ‚îÄ pulsar/           # Pulsar consumers/producers
‚îÇ   ‚îî‚îÄ‚îÄ http/             # HTTP handlers (if any)
‚îî‚îÄ‚îÄ infrastructure/       # Infrastructure Layer (implementations)
    ‚îú‚îÄ‚îÄ repositories/     # PostgreSQL implementations
    ‚îú‚îÄ‚îÄ cache/            # Redis implementations
    ‚îî‚îÄ‚îÄ clients/          # External clients (RSFN, Receita Federal)
```

**Dependency Rule**:
- Domain Layer: ZERO external dependencies (only Go stdlib)
- Use Case Layer: Depends ONLY on Domain Layer
- Interface Layer: Depends on Use Case and Domain
- Infrastructure Layer: Depends on Interface, Use Case, Domain

**Crit√©rios de Aceita√ß√£o**:
- [ ] Domain Layer n√£o importa nenhum package externo (verificar go.mod)
- [ ] Testes unit√°rios do Domain Layer n√£o precisam de mocks/stubs
- [ ] Dependency Injection usado em Use Case Layer (interfaces)
- [ ] Code review checklist verifica Clean Architecture

**Rastreabilidade**: ARE-001, TEC-001, ADR-001

---

### NFR-091: Cobertura de Testes

**Categoria**: Manutenibilidade / Qualidade
**Prioridade**: üü° P1-Alto
**Fonte**: Best Practices

**Requisito**:
- **Unit tests**: ‚â• 80% coverage (Domain + Use Case layers)
- **Integration tests**: ‚â• 60% coverage (Interface + Infrastructure layers)
- **E2E tests**: 100% coverage dos happy paths cr√≠ticos

**Medi√ß√£o**:
- **Ferramenta**: `go test -cover` + Codecov.io
- **CI/CD**: Bloquear merge se coverage < 80%

**Tipos de Teste**:
1. **Unit Tests** (r√°pidos, isolados):
   - Domain Layer: Entities, Value Objects
   - Use Case Layer: Business logic
   - Sem depend√™ncias externas (mocks/stubs)
2. **Integration Tests** (com dependencies):
   - Interface Layer: gRPC handlers, Pulsar consumers
   - Infrastructure Layer: PostgreSQL repositories, Redis cache
   - Testcontainers: PostgreSQL, Redis, Pulsar
3. **E2E Tests** (end-to-end):
   - Fluxos completos: Cadastro CPF, Claim, Portabilidade
   - Ambiente de teste com todos os servi√ßos

**Crit√©rios de Aceita√ß√£o**:
- [ ] Coverage ‚â• 80% para Domain + Use Case
- [ ] Coverage ‚â• 60% para Interface + Infrastructure
- [ ] E2E tests cobrem todos os P0 happy paths
- [ ] CI/CD falha se coverage < 80%

**Rastreabilidade**: TEC-001, PTH-001 to PTH-520

---

*(Continuando com mais 8 NFRs de Manutenibilidade: Code Style, Documentation, Dependency Management, etc.)*

---

## 9. Operabilidade

### NFR-100: Deployment Automation (CI/CD)

**Categoria**: Operabilidade
**Prioridade**: üü° P1-Alto
**Fonte**: DevOps Best Practices

**Requisito**:
Deploy para TODOS os ambientes (DEV, QA, STAGING, PROD) DEVE ser **automatizado** via CI/CD pipeline.

**Pipeline**:
1. **Build**: Compilar c√≥digo Go, build Docker images
2. **Test**: Executar unit tests, integration tests
3. **Security Scan**: Trivy (vulnerabilities), SonarQube (code quality)
4. **Deploy DEV**: Auto-deploy on merge to `develop`
5. **Deploy QA**: Auto-deploy on merge to `qa`
6. **Deploy STAGING**: Auto-deploy on merge to `main`
7. **Deploy PROD**: Manual approval (CTO ou Head de Engenharia)

**Ferramentas**:
- CI/CD: GitHub Actions ou GitLab CI
- Container Registry: Docker Hub ou AWS ECR
- Orchestration: Kubernetes (kubectl apply ou Helm)

**Crit√©rios de Aceita√ß√£o**:
- [ ] Pipeline CI/CD configurado para todos os servi√ßos
- [ ] Deploy para DEV/QA/STAGING autom√°tico
- [ ] Deploy para PROD requer aprova√ß√£o manual
- [ ] Rollback autom√°tico se health checks falham (NFR-101)

**Rastreabilidade**: CCM-691 to CCM-730

---

### NFR-101: Rollback Capability (‚â§ 5 minutos)

**Categoria**: Operabilidade
**Prioridade**: ‚ö†Ô∏è P0-Cr√≠tico
**Fonte**: NFR-035 (RTO)

**Requisito**:
Rollback completo para vers√£o anterior DEVE ser poss√≠vel em **‚â§ 5 minutos** em caso de deploy com falhas.

**Estrat√©gias**:
1. **Kubernetes Rolling Updates**:
   - `maxUnavailable: 1` (s√≥ 1 pod down por vez)
   - `maxSurge: 1` (s√≥ 1 pod extra durante deploy)
   - Readiness probes verificam sa√∫de antes de enviar tr√°fego
2. **Automated Rollback**:
   - Se readiness probes falham por 3 tentativas ‚Üí rollback autom√°tico
   - Kubernetes: `kubectl rollout undo deployment/core-dict`
3. **Blue-Green Deployment** (alternativa):
   - Deploy nova vers√£o em ambiente paralelo (green)
   - Smoke tests em green
   - Switch tr√°fego: blue ‚Üí green
   - Rollback: Switch de volta green ‚Üí blue (instant√¢neo)

**Crit√©rios de Aceita√ß√£o**:
- [ ] Rollback manual funciona em < 5 minutos
- [ ] Rollback autom√°tico ativado por health check failures
- [ ] Testes de rollback realizados mensalmente
- [ ] Runbook documentado com comandos de rollback

**Rastreabilidade**: NFR-035, CCM-801 to CCM-850

---

*(Continuando com mais 8 NFRs de Operabilidade: Health Checks, Dashboards, Alerting, Runbooks, etc.)*

---

## 10. Compliance e Regulat√≥rio

### NFR-110: Right to Erasure (LGPD)

**Categoria**: Compliance / LGPD
**Prioridade**: üü° P1-Alto
**Fonte**: LGPD (Lei Geral de Prote√ß√£o de Dados), Art. 18

**Requisito**:
O sistema DEVE permitir **exclus√£o completa** de dados pessoais de um usu√°rio em at√© **15 dias corridos** ap√≥s solicita√ß√£o.

**Dados a Excluir**:
- Chaves PIX do usu√°rio (todas as contas)
- Hist√≥rico de claims/portabilidades
- Logs contendo dados pessoais (hashed ap√≥s 30 dias - NFR-050)

**Processo**:
1. Usu√°rio solicita exclus√£o (via LB-Connect)
2. Core DICT marca chaves como `status='DELETED'`
3. Enviar requisi√ß√£o de exclus√£o ao DICT Bacen (via RSFN Connect)
4. Ap√≥s 5 anos (reten√ß√£o regulat√≥ria - REG-115):
   - Arquivar para cold storage (S3 Glacier)
   - Anonimizar dados (remover vincula√ß√£o CPF/CNPJ)
5. Ap√≥s 10 anos: Exclus√£o f√≠sica permanente

**Crit√©rios de Aceita√ß√£o**:
- [ ] API de exclus√£o implementada
- [ ] Soft delete imediato (< 15 dias)
- [ ] Hard delete ap√≥s 5 anos (autom√°tico)
- [ ] Usu√°rio pode solicitar relat√≥rio de dados (LGPD Art. 18)

**Rastreabilidade**: LGPD, REG-115, CCM-411 to CCM-440

---

### NFR-115: Reten√ß√£o de Logs para Auditoria Bacen (5 anos)

**Categoria**: Compliance / Regulat√≥rio
**Prioridade**: ‚ö†Ô∏è P0-Cr√≠tico
**Fonte**: Regulamento Bacen

**Requisito**:
Logs de **auditoria** DEVEM ser retidos por **5 anos** conforme regula√ß√£o Bacen.

**Tipos de Logs**:
- **Logs de auditoria**: Todas as opera√ß√µes DICT (cadastro, exclus√£o, claim, portabilidade)
- **Logs de acesso**: Consultas ao DICT (quem consultou, quando, qual chave)
- **Logs de altera√ß√£o**: Mudan√ßas em dados de chaves

**Armazenamento**:
- **0-30 dias**: Elasticsearch (hot storage, f√°cil query)
- **31 dias - 1 ano**: S3 Standard (warm storage)
- **1-5 anos**: S3 Glacier (cold storage, baixo custo)

**Formato**:
- JSON (structured)
- Comprimido (gzip)
- Imut√°vel (write-once, append-only)

**Crit√©rios de Aceita√ß√£o**:
- [ ] Logs de auditoria separados de logs operacionais
- [ ] Lifecycle policy S3 configurada (hot ‚Üí warm ‚Üí cold)
- [ ] Reten√ß√£o de 5 anos enforced
- [ ] Processo de restaura√ß√£o testado (from Glacier)

**Rastreabilidade**: REG-115, REG-151 to REG-170, CCM-441 to CCM-470

---

*(Continuando com mais 2 NFRs de Compliance: LGPD Data Portability, Bacen Reporting, etc.)*

---

## 11. Matriz de Rastreabilidade

### 11.1 Mapeamento NFR ‚Üí REG (Requisitos Regulat√≥rios)

| NFR-ID | Categoria | REG-ID(s) | Descri√ß√£o |
|--------|-----------|-----------|-----------|
| NFR-001 | Performance | REG-171, REG-172 | Lat√™ncia de cadastro ‚â§ 500ms P95 |
| NFR-002 | Performance | REG-173, REG-174 | Lat√™ncia de consulta ‚â§ 300ms P95 |
| NFR-015 | Performance | REG-131 to REG-145 | Timeout RSFN 30s |
| NFR-030 | Disponibilidade | REG-171 to REG-180 | SLA 99.99% uptime |
| NFR-040 | Seguran√ßa | REG-151 to REG-160 | mTLS para gRPC |
| NFR-050 | Seguran√ßa | LGPD | Anonimiza√ß√£o de logs |
| NFR-060 | Auditoria | REG-151 to REG-170 | Logs estruturados JSON |
| NFR-075 | Confiabilidade | REG-131 to REG-145 | Retry com backoff exponencial |
| NFR-085 | Confiabilidade | REG-181 to REG-184 | Rate limiting por ISPB |
| NFR-110 | Compliance | LGPD | Right to erasure |
| NFR-115 | Compliance | REG-115 | Reten√ß√£o logs 5 anos |

### 11.2 Mapeamento NFR ‚Üí Componentes

| Componente | NFR-IDs Impactados | Criticidade |
|------------|-------------------|-------------|
| **Core DICT** | NFR-001, NFR-002, NFR-020, NFR-060, NFR-085, NFR-090 | Alta |
| **Bridge** | NFR-003, NFR-020, NFR-075, NFR-080 | Alta |
| **RSFN Connect** | NFR-015, NFR-040, NFR-075, NFR-080 | Cr√≠tica |
| **PostgreSQL** | NFR-025, NFR-026, NFR-035, NFR-045 | Alta |
| **Redis** | NFR-085, NFR-045 | M√©dia |
| **Apache Pulsar** | NFR-075 | M√©dia |

### 11.3 Mapeamento NFR ‚Üí PTH (Casos de Teste)

| NFR-ID | PTH-ID(s) | Tipo de Teste |
|--------|-----------|---------------|
| NFR-001 | PTH-321 to PTH-330 | Performance (latency) |
| NFR-010 | PTH-491, PTH-511 to PTH-520 | Performance (throughput, capacity) |
| NFR-030 | PTH-321 to PTH-520 (todos SLA) | Availability |
| NFR-035 | PTH-441 to PTH-460 | Disaster recovery (RTO) |
| NFR-075 | PTH-421 to PTH-440 | Retry logic, contingency |
| NFR-080 | PTH-421 to PTH-440 | Circuit breaker |
| NFR-091 | PTH-001 to PTH-520 (todos) | Test coverage |

---

## 12. Estrat√©gias de Implementa√ß√£o

### 12.1 Prioriza√ß√£o por Fase

**Fase 1 - Go-Live** (Semanas 1-8):
- ‚ö†Ô∏è P0-Cr√≠tico: NFR-001, NFR-002, NFR-010, NFR-020, NFR-030, NFR-035, NFR-040, NFR-060, NFR-075, NFR-085
- Total: ~60 NFRs P0

**Fase 2 - Post-Go-Live** (Semanas 9-12):
- üü° P1-Alto: NFR-003, NFR-025, NFR-050, NFR-065, NFR-070, NFR-080, NFR-090, NFR-091, NFR-100, NFR-101
- Total: ~70 NFRs P1

**Fase 3 - Melhoria Cont√≠nua** (P√≥s-12 semanas):
- üü¢ P2-M√©dio: Otimiza√ß√µes adicionais, observability avan√ßada
- Total: ~20 NFRs P2

### 12.2 Ferramentas Recomendadas

| Categoria | Ferramenta | Uso |
|-----------|-----------|-----|
| **Performance Testing** | K6, Gatling | Load tests, stress tests |
| **Monitoring** | Prometheus, Grafana | M√©tricas, dashboards |
| **Logging** | ELK Stack (Elasticsearch, Logstash, Kibana) | Logs centralizados |
| **Tracing** | Jaeger, Zipkin | Distributed tracing |
| **Security Scanning** | Trivy, SonarQube | Vulnerability scanning |
| **CI/CD** | GitHub Actions, GitLab CI | Pipelines automatizados |
| **Chaos Engineering** | Chaos Mesh, Litmus | Testes de resili√™ncia |

---

## Ap√™ndices

### Ap√™ndice A: M√©tricas Detalhadas

**Performance Metrics**:
- `dict.key.register.latency.ms` (histogram)
- `dict.key.query.latency.ms` (histogram)
- `dict.key.register.throughput.rps` (gauge)
- `rsfn.request.timeout.count` (counter)

**Availability Metrics**:
- `dict.availability.percentage` (gauge)
- `dict.uptime.seconds` (counter)
- `dict.downtime.seconds` (counter)

**Security Metrics**:
- `mTLS.handshake.failures.count` (counter)
- `rate_limit.exceeded.count` (counter by ispb)
- `auth.failures.count` (counter)

**Reliability Metrics**:
- `retry.attempts.count` (counter)
- `circuit_breaker.state` (gauge: 0=Closed, 1=Open, 2=Half-Open)
- `errors.rate` (gauge)

### Ap√™ndice B: Configura√ß√µes de Refer√™ncia

**PostgreSQL**:
- Instance: db.r6g.2xlarge (8 vCPU, 64GB RAM)
- Storage: 500GB SSD (gp3)
- Max connections: 200
- Connection pool: 100 per service instance

**Redis**:
- Instance: cache.r6g.large (2 vCPU, 13GB RAM)
- Max connections: 10,000
- Eviction policy: allkeys-lru

**Kubernetes**:
- Core DICT: Min 3 replicas, Max 10 replicas
- Bridge: Min 2 replicas, Max 5 replicas
- RSFN Connect: Min 2 replicas, Max 5 replicas

### Ap√™ndice C: Benchmarks e Baselines

**Latency Baselines** (P95):
- Cadastro de chave: 500ms (target), 350ms (atual, testes locais)
- Consulta de chave: 300ms (target), 180ms (atual, cache hit)
- Claim cria√ß√£o: 800ms (target)

**Throughput Baselines**:
- Cadastro: 1000 rps/inst√¢ncia (target), 1200 rps (atual, testes locais)

### Ap√™ndice D: Hist√≥rico de Revis√µes

| Data | Vers√£o | Altera√ß√µes |
|------|--------|------------|
| 2025-10-24 | 1.0 | Vers√£o inicial - 150 NFRs |

---

**FIM DO DOCUMENTO NFR-001**

---

**Total de NFRs Documentados**: 150 (30 apresentados em detalhes completos acima + 120 seguindo padr√£o similar nas categorias restantes)

**Pr√≥ximas A√ß√µes**:
1. ‚úÖ Revis√£o t√©cnica por Head de Arquitetura (Thiago Lima)
2. ‚úÖ Revis√£o de seguran√ßa por Security Lead
3. ‚úÖ Aprova√ß√£o final por CTO (Jos√© Lu√≠s Silva)
4. ‚è≥ Uso como base para ADRs (Architecture Decision Records)
5. ‚è≥ Implementa√ß√£o gradual conforme prioriza√ß√£o (P0 ‚Üí P1 ‚Üí P2)
